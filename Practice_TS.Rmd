---
title: "Time Series"
author: "Akash Mittal"
date: "2025-08-12"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
# terminal commands to push to github 
# git remote add origin https://github.com/sky-akash/Time-Series.git
# git push -u origin master

# want to know what changed between committs ?
# git log c5e3719..3c5a3e8
# git log --oneline c5e3719..3c5a3e8
# ? what changed from commit A (c5e3719) to commit B (3c5a3e8).

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax
for authoring HTML, PDF, and MS Word documents. For more details on
using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. You can embed an R code chunk like this:

```{r cars}
# ye naam kyun dena ?
#  Naam dene ke 5 faayde:
# 1. Error messages ko samajhne mein aasani hoti hai
# 2. Document ke structure ko samajhna easy ho jaata hai
# 3. Caching ke liye zaroori hota hai
#     {r clean-data, cache=TRUE}
# 4. Output files (like images) ka naam control karna
#     {r scatter-plot}
#     plot(cars)
#     O/p Name will be : scatter-plot-1.png
# 5. Cross-referencing ke liye
#     “See analysis in chunk model-fit”
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
# Code ko output document mein nahi dikhana, lekin output (jaise plot) ko dikhana.
# 
# In simple terms:
# Code chalega ✔️
# 
# Code output aayega ✔️
# 
# Code khud nahi dikhega ❌

#plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to
prevent printing of the R code that generated the plot.

------------------------------------------------------------------------

```{r}
#install.packages("fpp3", dependencies = TRUE)

library(fpp3)         # Loads a collection of packages (including tsibble, fable, feasts) for forecasting and time series analysis.

library(fable)        # Used for forecasting models (like ARIMA, ETS, etc.) with tidy syntax.

library(tibble)       # A modern reimagining of data frames that is more consistent and user-friendly.

library(tsibble)      # Provides a time-aware data structure (tsibble) for tidy time series analysis.

library(feasts)       # Tools for time series feature extraction and decomposition (like STL, ACF, etc.).

library(tsibbledata)  # Provides sample time series datasets (such as `aus_retail`, `global_economy`, etc.).

library(tidyr)        # Helps in tidying data (reshaping, pivoting, separating columns, etc.).

library(ggplot2)      # A popular and flexible system for data visualization using the grammar of graphics.

library(lubridate)    # Makes working with dates and times easier (e.g., parsing, extracting components like year/month).

library(dplyr)        # Used for data manipulation (filtering, selecting, mutating, summarizing, etc.).

library(fabletools)   # Provides tools and infrastructure to support modeling with fable (under the hood utilities).

library(slider)
library(seasonal)
```

## Time Series Graphics

```{r avialable datasets}

data() # lists abvailable datasets (A lot of them in these packages....)

```

```{r aus_retail dataset}

# summary(aus_retail)
str(aus_retail)

```

```{r unique-items}

# aus_retail |> distinct('Series ID')
# 
# aus_retail |> unique('State') # issue as unique wants a vector, and can't evven do aus_retail$'State' as It makes me pass df again to original df

aus_retail |> 
  pull(State) |>
  unique()

# OR 

unique(pull(aus_retail, State))

# OR 

unique(aus_retail$State)      

# OR Use distinct from (deplyr)

aus_retail |> distinct(State) |> pull()             # All aboe ancd this doens give output as tibble but just "A VECTOR"

aus_retail |> distinct(State) # here even output is a tibble

# ###########

aus_retail |> distinct(`Series ID`)
aus_retail |> distinct(State)
aus_retail |> distinct(Industry)

colnames(aus_retail)
head(aus_retail)


aus_retail |> distinct(State, Industry) # distinct also used to in combined series too

```

```{r}

aus_retail |>
  filter(`Series ID` == 'A3349640L') |>
  autoplot(Turnover)

```

```{r}
aus_retail |> 
  filter(`Series ID` == "A3349640L") |>
  model(ETS(Turnover)) |>
  forecast(h = '2 year')

```

```{r}
aus_retail |> 
  filter(`Series ID` == "A3349640L") |>
  model(ETS(Turnover)) |>
  forecast(h = '2 years')
```

```{r ch1}

y <- tsibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110),
  index = Year
)

plot(y)

y

```

```{r}
# converting a dataset to tsibble

# z = data.frame("Month" = "2019 Jan" to "2019 May", # won't work as need to individually write the month year data
#                "observation" = c(50, 23, 45, 34, 67))

z = data.frame(
  "Month" = seq(yearmonth("2019 Jan"), yearmonth("2019 May"), by = 1),
  "observation" = c(50, 23, 45, 34, 67))


z

z |> 
  mutate(Month = yearmonth(Month)) |>
  as_tsibble(index=Month)
# Not yet assigned back to z, so z will still be a df not a tsibble

z

# Notes on ts data
# 1. Creating or cleaning time columns
# If you have a column like "2019 Jan" or "2020 Q2" in character format, you'll use:
# 
# Frequency	Use this function	Example
# Monthly	yearmonth()	yearmonth("2019 Jan")
# Quarterly	yearquarter()	yearquarter("2020 Q2")
# Weekly	yearweek()	yearweek("2021 W05")
# Daily	ymd() or as_date()	ymd("2021-08-12")
# Hourly+	ymd_hms() or as_datetime()	ymd_hms("2021-08-12 13:45:00")

# 2. Converting to a tsibble
# as_tsibble(index = time_column) # time_column must of class mentioned above or POSIXct

# 3. Creating date sequences
# seq(yearmonth("2019 Jan"), yearmonth("2020 Jan"), by = 1)
# seq(yearquarter("2019 Q1"), yearquarter("2020 Q1"), by = 1)
# seq(ymd("2022-01-01"), ymd("2022-12-31"), by = "1 day")

#### Notice the difference between how by is passed for difference base values of time


```

```{r}
# key variables in time series of tsibble 
# allows multiple time series to be stored in a single object
#

olympic_running


# 312*4 [4Y] - 312 rows, 4 columns and period of 4 years
# key -> length, sex [14] -> means 14 differencet time series are there
# The 14 time series in this object are uniquely identified by the keys: the Length and Sex variables.
#

olympic_running |>
  distinct(Length, Sex)

```

```{r deplyr functions}

# mutate(), filter(), select() and summarise() on tsibble object

PBS

key_vars(PBS)

PBS |>
  filter(ATC1=="A")

PBS |>
  filter(ATC1=="A") |>
  select(Month, Concession, Type)

PBS |>
  filter(ATC1=="A") |>
  select(Month, Concession, Type, Cost) |>
  summarize(TotalC=sum(Cost))


PBS |>
  filter(ATC1 == "A") |>
  select(Month, Concession, Type, Cost) |>
  summarize(TotalC = sum(Cost)) |>
  mutate(Cost = TotalC/1e6)

# saving the tsibble

PBS |>
  filter(ATC2 == "A10") |>
  select(Month, Concession, Type, Cost) |>
  summarise(TotalC = sum(Cost)) |>
  mutate(Cost = TotalC / 1e6) -> a10          # saving the tsibble


a10
```

```{r}
has_gaps(PBS)   # Are there any missing time points? # to check for any missing time series data momtnths etc.
# .gaps = FALSE → No months are missing; the time series is complete.
# .gaps = TRUE → Some months are missing in the time series — there's a gap in the timeline.
############

# if any gaps can fill using """""""""fill_gaps()"""""" or """"""na_interpolate()""""""

############
n_keys(PBS)     # How many unique series?
```

```{r csv_to_tsibble}
#

prison <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/prison_population.csv")

summary(prison)
str(prison)

# spc_tbl_ [3,072 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
#  $ Date      : Date[1:3072], format: "2005-03-01" "2005-03-01" "2005-03-01" "2005-03-01" ...
#  $ State     : chr [1:3072] "ACT" "ACT" "ACT" "ACT" ...
#  $ Gender    : chr [1:3072] "Female" "Female" "Female" "Female" ...
#  $ Legal     : chr [1:3072] "Remanded" "Remanded" "Sentenced" "Sentenced" ...
#  $ Indigenous: chr [1:3072] "ATSI" "Non-ATSI" "ATSI" "Non-ATSI" ...
#  $ Count     : num [1:3072] 0 2 0 5 7 58 5 101 51 131 ...
#  - attr(*, "spec")=
#   .. cols(
#   ..   Date = col_date(format = ""),
#   ..   State = col_character(),
#   ..   Gender = col_character(),
#   ..   Legal = col_character(),
#   ..   Indigenous = col_character(),
#   ..   Count = col_double()
#   .. )
#  - attr(*, "problems")=<externalptr> 


prison <- prison |>
  mutate(Quarter = yearquarter(Date)) |>
  select(-Date) |>
  as_tsibble(key = c(State, Gender, Legal, Indigenous),
             index = Quarter)
prison


# More complicated (and unusual) seasonal patterns can be specified using the "period()" function in the "lubridate" package.

```

```{r}
ansett
```

```{r Time plots}

melsyd_economy <- ansett |>
  filter(Airports == "MEL-SYD", Class=="Economy")|>
  mutate(Passengers = Passengers/1000)

melsyd_economy

autoplot(melsyd_economy,Passengers) + # dataset and the column data which has to be plotted
  labs(title = "Ansett airlines economy class",
       subtitle = 'Melbourne-Sydney',
       y = "Passengers ('000) i.e. in Thousands")

```

```{r}

a10

str(a10)
#summary(a10)

autoplot(a10, Cost) +
  labs(y = "Million dollars",
       title = "Australian drug Sales")

a10 |> 
  gg_season(Cost, labels = 'both') +
  labs(y = "Millions Dollars",
       title = "Seasonal Plot : Antidiabetic Drug Sales")

# making seasonal plot, so it automatically plotted the lines as per every year and different lines for each month as data is monthly seasonal


```

```{r}
# data which is seasonal by day

vic_elec

str(vic_elec)
# i.e. half hourly data is given

vic_elec |> 
  gg_season(Demand, period = "day")  +           #  If a string (e.g., "1y" for 1 year, "3m" for 3 months,
                                                # "1d" for 1 day, "1h" for 1 hour, "1min" for 1 minute, "1s"
                                                # for 1 second), it's converted to a Period class object from
                                                # the lubridate package. Note that the data must have at least
                                                # one observation per seasonal period, and the period cannot be
                                                # smaller than the observation interval.
  theme(legend.position = "none") + 
  labs(y = "MWh", title = "Electricity Demand : Victoria")



```

```{r}
# seasonal plots

vic_elec

vic_elec |> gg_season(Demand, period = "week") +
  theme(legend.position = "none") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec |> gg_season(Demand, period = "month") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec |> gg_season(Demand, period = "year") +
  labs(y="MWh", title="Electricity demand: Victoria")

```

```{r}

# # vic_elec |> gg_season(Demand, period = "month")
# # Cannot pass series as strings in gg_subseries , it re
# vic_elec |> gg_subseries(Demand, period = 336) +
#   labs(y = "Million dollars",
#        title = "Australian antidiabetic drug sales")

```

Subseries plots

```{r}
a10

str(a10)
```

```{r}
a10 |>
  gg_season(Cost)

```

```{r subseries plots seasonals}

a10 |>
  gg_subseries(Cost) +
  labs(
    y = "$ (millions)",
    title = "Australian antidiabetic drug sales"
  )

```

```{r tourism australian - sub_series vs sub_seasonal plots}

tourism

str(tourism)

# tbl_ts [24,320 × 5] (S3: tbl_ts/tbl_df/tbl/data.frame)
#  $ Quarter: qtr [1:24320] 1998 Q1, 1998 Q2, 1998 Q3, 1998 Q4, 1999 Q1, 1999 Q2, 1999 Q3, 1999 Q4, 2000 Q1, 2000 Q2, ...
#    ..@ fiscal_start: num 1
#  $ Region : chr [1:24320] "Adelaide" "Adelaide" "Adelaide" "Adelaide" ...
#  $ State  : chr [1:24320] "South Australia" "South Australia" "South Australia" "South Australia" ...
#  $ Purpose: chr [1:24320] "Business" "Business" "Business" "Business" ...
#  $ Trips  : num [1:24320] 135 110 166 127 137 ...

# tourism is tsibble data with Quarter as time frames..so while grouping the hierarchy is State -> Region
# grouping by state we get data for 

holidays <- tourism |>
  filter(Purpose == 'Holiday') |>
  group_by(State) |>
  summarise(Trips=sum(Trips))

holidays

autoplot(holidays, Trips)+
  labs(y = "Trips (thousands") +
  labs(title = "Australian domestic holidays")


gg_season(holidays, Trips) +                      # sub seasonal plot ... and there are eight states (as)
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

holidays |>                                       
  gg_subseries(Trips) +                           # sub series plots ..... time frame is quarterly, and
                                                  # thus 4 quarters for each state
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

```

```{r}
holidays
```

```{r}

vic_elec |>
  filter(year(Time) == 2014) |>
  autoplot(Demand) +
  labs(y = "GW",
       title = "Half-hourly electricity demand: Victoria")

```

```{r}
vic_elec

vic_elec |>
  filter(year(Time) == 2014) |>
  autoplot(Temperature) +
  labs(
    y = "deg C",
    title = "Half Hourly Temperatures (Melbourne)"
  )

# Note: In the dataset, the temperatures are for Melbourne, the largest city in Victoria, while the demand values are for the entire state.

```

```{r}
# scatterplot of series Temp vs demand

vic_elec |>
  filter(year(Time)==2014) |>
  ggplot(aes(x=Temperature, y=Demand)) +
  geom_point() + 
  labs(
    title = "Electricity Demand vs Temp.",
    x = "Temp. deg C Melbourne",
    y = "Electricity  Demand in Victoria"
  )


```

```{r}
# Scatterplot Matrices


visitors <- tourism |>
  group_by(State) |>
  summarise(Trips = sum(Trips))


visitors |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State), scales = "free_y") +
  labs(title = "Australian domestic tourism",
       y= "Overnight trips ('000)")


visitors |>
  pivot_wider(values_from=Trips, names_from=State) |>
  GGally::ggpairs(columns = 2:9)

```

```{r}

unique(visitors$State)

```

```{r lag plots}

aus_production 
str(aus_production)
#summary(aus_production) - error isssue with time series

```

```{r}

recent_production <- aus_production |>
  filter(year(Quarter) >= 1990)

recent_production |>
  feasts::gg_lag(Beer, geom = "path", lags = 1:10, arrow=TRUE) +                   # geom can be "point" or "path" # by fefault there are 9 lags 
  labs(x = "lag(Beer, k)")

```

```{r autocorrelations}

recent_production |>
  feasts::ACF(Beer, lag_max = 50)
  

recent_production |>
  feasts::ACF(Beer, lag_max = 50) |>                        # this will give a tsibble df with lags and acf (And autoplot will
                                        # plot it accordingly)
  autoplot() + 
  labs(title = "Australian Beer Production")
  
  

```

-   peaks in 4 quarters and troughs too.
-   dashed blue line -\> if correlations are significantly differente
    from 0 or not. "Bartlett Bands"

```{r trend and seasonality in ACF}

a10
str(a10)    #  3 monthly seasonal


a10 |>
  feasts::ACF(Cost, lag_max = 50) |>
  autoplot() +
  labs(title="ACT Anti")



```

<!-- exponentially decreasing acf with peaks at every 12 months apart..better check the PACF plots too.  -->

```{r drawing PACF}
a10 |>
  feasts::PACF(Cost, lag_max = 50) |>
  autoplot() +
  labs(title="ACT Anti")

```

```{r White Noise}
set.seed(3)
y <- tsibble(
  sample = 1:50,
  wn = rnorm(50),
  index = sample
)

y 

y |> autoplot(wn) +
  labs(title = "White Noise", y = "")               # y = "" removes label from y axis


y |>
  ACF(wn) |>
  autoplot() + labs(title="White Noise ACF")


```

## Time Series Decomposition

```{r transformation and adjustment}

global_economy

str(global_economy)

tsibbledata::global_economy |>
  filter(Country == "Australia") |>
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y="$US")


global_economy

str(global_economy)

tsibbledata::global_economy |>
  filter(Country == "India") |>
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y="$US")

```

```{r inflation price adjustments}

print_retail <- aus_retail |>
  filter(Industry == "Newspaper and book retailing") |>
  group_by(Industry) |>
  index_by(Year = year(Month)) |>
  summarise(Turnover = sum(Turnover))  # |> mutate(Industry = "Newspaper and book retailing")

print_retail              # newspaper printing turnover data over years
  
aus_economy <- global_economy |>
  filter(Code == "AUS")

aus_economy               # australian economy data oer years including CPI

# both have different years and also we need to mathc data for years so we perform left join to have consistent data

print_retail |>
  left_join(aus_economy, by = "Year") |> 
  mutate(`Inf. Adjusted Turnover` = Turnover/CPI * 100) # |>
  #pivot_longer(c(Turnover, `Inf. Adjusted Turnover`),
   #            values_to = "Turnover")
print_retail

```

```{r}

print_retail |>
  left_join(aus_economy, by = "Year") |> 
  mutate(`Inf. Adjusted Turnover` = Turnover/CPI * 100) |>
  pivot_longer(c(Turnover, `Inf. Adjusted Turnover`), # by long format we create a column with name "name" and values given in this vector, we can remname the coum too, using names_to = `Turnover Type`
              values_to = "Turnover") |>  # turnover is just a name of new column
  mutate(name=factor(name, 
                     levels=c('Turnover', 'Inf. Adjusted Turnover'))) |>
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() + 
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry",
       y = "$AU")
  
  
print_retail


```

Box COX Transformation (allows for negative values which log transform
doesn't) (also helps in converting data to normal form (not always))

```         
  |
  | log(Yt)      if λ = 0  (for log 0, ie. zero remainss zero)
```

Wt = \| \| ((sign(Yt)/Yt)\^(λ) - 1) / λ else \| OR \| (Yt)\^(λ) - 1) /
λ) \# variation neeeds to know difference

A good value of λ is one which makes the size of the seasonal variation
about the same across the whole series, as that makes the forecasting
model simpler.

```{r BoX Cox Transformation & guerrero feature}

# lambda is found using guerrero feature

aus_production |>
  autoplot(Gas)

lambda <- aus_production |>
  fabletools::features(Gas, features = guerrero) |>
  pull(lambda_guerrero)


aus_production |>
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0(
         "Transformed gas production with $\\lambda$ = ",
         round(lambda,2))))



```

```{r Time series components, STL decomposition}

fpp3::us_employment # monthly seasonal with Series_ID as having 148 different series data

us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == 'Retail Trade') |> # title named coumn with valeus being Retail Trade
  select(-Series_ID)

us_retail_employment

autoplot(us_retail_employment,Employed)+
  labs(y = "Persons (thousands)",
       title = "Total Employment in US Retail")


```

<!-- # looks to have trend and some seasonality maybe ? let's divide into parts and check | -->

```{r decomposition}

dcmp <- us_retail_employment |>
  fabletools::model(
                    stl = STL(Employed)
                    )
fabletools::components(dcmp)

str(components(dcmp))

```

<!-- The output above shows the components of an STL decomposition. -->

```{r}

str(components(dcmp))

components(dcmp) |>
  as_tsibble()

components(dcmp) |>
  as_tsibble() |>                                                 # components(dcmp) is a dataframe so need to                                                                         # convert to tsibble
  autoplot(Employed, colour="pink") +
  geom_line(aes(y=trend), colour="darkgreen") +
  labs(
    y = "Persons (k)",
    title = "Total Employment in US Retail (Trend)"
  )

# Seasonally Adjusted Data

# data after removing seasonal compoenet from data

components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail (Seasonally Adjusted)")

```

<!-- Seasonally adjusted plot is wiggly, because it has Trend component and the remainder compoentns (noise) -->

```{r plotting all components}

components(dcmp) |> autoplot()           # here we didn't need it to be tsibble object ?

# lets try by converting it to tsibble

components(dcmp) |>
  as_tsibble() |>
  autoplot()                 # doesn't wor kas required !

# alternative
components(dcmp) |>
  as_tsibble() |>
  pivot_longer(cols = c(trend, season_year, remainder), names_to = "component") |>
  ggplot(aes(x = Month, y = value)) +                   # "|" inside ggplot is used fro faceting/ facets so diff
                                                          # plot  for each component
  geom_line() + 
  facet_wrap(~ component, scales="free_y") + 
  labs(
    y = "Value",
    title = "Decomposed Components"
  ) + 
  theme_minimal()



```

<!-- The grey bars to the right of each panel show the relative scales of the components. Each grey bar represents the same length but because the plots are on different scales, the bars vary in size. -->

```{r}


```

```{r moving averages}

global_economy

global_economy |>
  filter(Country == "Australia")

global_economy |>
  filter(Country == "Australia") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")


################################################

aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE),
    `3-MA` = slider::slide_dbl(Exports, mean,
                .before = 1, .after = 1, .complete = TRUE),
    `7-MA` = slider::slide_dbl(Exports, mean,
                .before = 3, .after = 3, .complete = TRUE)
  ) # 5 period moving average so there will be 2 less observations on top and two less en end

aus_exports

aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "red") +
  geom_line(aes(y = `7-MA`), colour = "green") +
  geom_line(aes(y = `3-MA`), colour = "yellow") +
  labs(y = "% of GDP",
       title = "Total Australian exports")

# slide(.x, .f, ..., .before = 0L, .after = 0L, .step = 1L, .complete = FALSE)
# sliding on a, applying function f.

```

```{r Moving Average of Moving Averages}

aus_production # Quarterly Data

beer <- aus_production |>
  filter(year(Quarter) >= 1992) |>
  select(Quarter, Beer)


beer_ma <-
  beer |>
  mutate(
  
    `4-MA` = slider::slide_dbl(Beer, mean,
                .before = 1, .after = 2, .complete = TRUE),
    
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )

# > >   Y(1)
# > >...Y(1,2) 4MA(1)
#         but 4MA(1) is for this line in real              
# > >   y(1,2) 4MA(2)                                 2*4MA (i.e. average of 2.5 and 2.5 is 3)
#         but 4MA(2) is for this line in real               
# > >   Y(1,2) 
# > >   Y(2)
# > > 

# Lets plot it

beer_ma |>
  autoplot(`Beer`) +
  geom_line(aes(y = `2x4-MA`), colour = "purple")


```

```{r trend-cycle + seasonal }

us_retail_employment # monthly seasonal

us_retail_employment_ma <- us_retail_employment |>
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )


us_retail_employment_ma |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")

```

<!-- Classical decomposition -->

<!-- Additive decomposition \ -->

<!-- Step 1 -->
<!-- If $m$ is an even number, compute the -->
<!-- trend-cycle component $\hat{T}_t$ using a $2 \times m$-MA. If $m$ is an -->
<!-- odd number, compute the trend-cycle component $\hat{T}_t$ using an -->
<!-- $m$-MA. -->

<!-- Step 2 -->
<!-- Calculate the detrended series: $y_t - \hat{T}_t$. -->

<!-- Step 3 -->
<!-- To estimate the seasonal component for each season, simply average the -->
<!-- detrended values for that season. For example, with monthly data, the -->
<!-- seasonal component for March is the average of all the detrended March -->
<!-- values in the data. These seasonal component values are then adjusted to -->
<!-- ensure that they add to zero. The seasonal component is obtained by -->
<!-- stringing together these monthly values, and then replicating the -->
<!-- sequence for each year of data. This gives $\hat{S}_t$. -->

<!-- Step 4 -->
<!-- The remainder component is calculated by subtracting the estimated -->
<!-- seasonal and trend-cycle components: $$ -->
<!-- \hat{R}_t = y_t - \hat{T}_t - \hat{S}_t -->


```{r Classical decomposition}

# feasts pacakege function classical_decomposition
# type - additive or multiplicative
# classical_decomposition(formula, type = c("additive", "multiplicative"), ...)
# ... Other arguments passed to stats::decompose().

us_retail_employment |>
  model(
    feasts::classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

 # earlier we used ... STL Decomposition
# fabletools::model(
#                     stl = STL(Employed)
#                     )
# fabletools::components(dcmp)

```

<!-- Multiplicatie Decomposition -->
<!-- Step 1 -->
<!-- If \( m \) is an even number, compute the trend-cycle component \( \hat{T}_t \) using a \( 2 \times m \)-MA. If \( m \) is an odd number, compute the trend-cycle component \( \hat{T}_t \) using an \( m \)-MA. -->

<!-- Step 2 -->
<!-- Calculate the detrended series:   -->
<!-- \[ -->
<!-- \frac{y_t}{\hat{T}_t} -->
<!-- \] -->

<!-- Step 3 -->
<!-- To estimate the seasonal component for each season, simply average the detrended values for that season. For example, with monthly data, the seasonal index for March is the average of all the detrended March values in the data. These seasonal indexes are then adjusted to ensure that they add to \( m \). The seasonal component is obtained by stringing together these monthly indexes, and then replicating the sequence for each year of data. This gives \( \hat{S}_t \). -->

<!-- Step 4 -->
<!-- The remainder component is calculated by dividing out the estimated seasonal and trend-cycle components:   -->
<!-- \[ -->
<!-- \hat{R}_t = \frac{y_t}{\hat{T}_t \hat{S}_t} -->
<!-- \] -->

```{r Multiplicative decomposition}

us_retail_employment |>
  model(
    feasts::classical_decomposition(Employed, type = "multiplicative")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical Multiplicative decomposition of total
                  US retail employment")

```

<!-- Visually  dont see major different in curves but see the difference in grey bars for Employed and Trend -->

<!-- # X11 and SEATS -->

X-11 -> The X-11 method uses weighted averages over a moving window of the time series. This is used in combination with the RegARIMA model to prepare the data for decomposition. To use the X-11 decomposition method, the x11() function can be used in the model formula

SEATS -> default method for seasonally adjusting the data. This decomposition method can extract seasonality from data with seasonal periods of 2 (biannual), 4 (quarterly), 6 (bimonthly), and 12 (monthly). This method is specified using the seats() function in the model formula.

```{r x11 method of decomposition}

# Deeloped by Official statistics agencies (such as the US Census Bureau and the Australian Bureau of Statistics)

# Most of them use variants of the X-11 method, or the SEATS method, or a combination of the two. These methods are designed specifically to work with quarterly and monthly data, which are the most common series handled by official statistics agencies. They will not handle seasonality of other kinds, such as daily data, or hourly data, or weekly data. 

# There are methods for both ADDITIVE and MULTIPLICATIVE decomposition. 

# x11 decomposition is in X-13ARIMA-SEATS  function of feasts package
# but we also installed seasonal package for this

us_retail_employment

x11_dcmp <- us_retail_employment |>
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) |>
  components()
autoplot(x11_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using X-11.")

#######################################

x11_dcmp

x11_dcmp |>
  ggplot(aes(x = Month)) +
  geom_line(aes(y = Employed, colour = "Data")) +
  geom_line(aes(y = season_adjust,
                colour = "Seasonally Adjusted")) +
  geom_line(aes(y = trend, colour = "Trend")) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail") +
  scale_colour_manual(
    values = c("gray", "#0072B2", "#D55E00"),
    breaks = c("Data", "Seasonally Adjusted", "Trend")
  )

######################################
#         Subseries Plots            #
######################################
x11_dcmp |>
  gg_subseries(trend)

x11_dcmp |>
  gg_subseries(seasonal)

```

```{r SEATS method - Seasonal Extraction in ARIMA Time Series}

seats_dcmp <- us_retail_employment |>
  model(seats = X_13ARIMA_SEATS(Employed ~ seats())) |>
  components()
autoplot(seats_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using SEATS")

# The results are similar though
```

```{r STL Decomposition}

us_retail_employment        # Monthly Series

#################################################################################
#   ?STL     -> Multiple seasonal decomposition by Loess  | from feasts
#   Decompose a time series into seasonal, trend and remainder components. Seasonal components are estimated iteratively using STL. Multiple seasonal periods are allowed. The trend component is computed for the last iteration of STL. Non-seasonal time series are decomposed into trend and remainder only. In this case, supsmu is used to estimate the trend. Optionally, the time series may be Box-Cox transformed before decomposition. Unlike stl, mstl is completely automated.
# 
# STL(formula, iterations = 2, ...)
# trend(window, degree, jump)
# season(period = NULL, window = NULL, degree, jump) | If the window is set to "periodic" or Inf, the seasonal pattern will be fixed. 
#
#################################################################################

us_retail_employment |>
  model(
    feasts::STL(Employed ~ trend(window = 7) +           # span(in lags)of loess window (need be ODD)
                   season(window = "periodic"),         # window is set to "periodic" or Inf, the                                                             seasonal pattern will be fixed.
    robust = TRUE)) |>
  components() |>
  autoplot()

#
#
#  The two main parameters to be chosen when using STL are the trend-cycle window trend(window = ?) and the seasonal window season(window = ?). These control how rapidly the trend-cycle and seasonal components can change. Smaller values allow for more rapid changes. Both trend and seasonal windows should be odd numbers; trend window is the number of consecutive observations to be used when estimating the trend-cycle; season window is the number of consecutive years to be used in estimating each value in the seasonal component. Setting the seasonal window to be infinite is equivalent to forcing the seasonal component to be periodic season(window='periodic') (i.e., identical across years). 
#
#########################################
```


## Time series features

<!-- The feasts package includes functions for Feature Extraction And Statistics from Time Series (hence the name) -->



```{r descriptive features}

# dataset
tourism  # quarterly REgion + State + Purpose + Trips

tourism |>
  features(Trips, list(mean=mean, sd=sd, quantiles=quantile)) |>
  arrange(mean)                                           # will sort in ascending order of mean

#other way

tourism |>
  features(Trips, quantile)


```


```{r ACF features}
# A time series decomposition can be used to measure the strength of trend and seasonality in a time series

# when we have a large collection of time series, and need to find the series with the most trend or the most seasonality

# feat_acf()

tourism |>
  features(Trips, feat_acf)

colnames(tourism |>
  features(Trips, feat_acf)
)

# using these features in plots to identify what type of series are heavily trended and what are most seasonal.
```

```{r}

tourism |>
  features(Trips, feat_stl)

colnames(tourism |>
  features(Trips, feat_stl))


#####

tourism |>
  features(Trips, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))

```
Holiday series are most seasonal.

```{r}
# identifying most seasonal series and plotting it


tourism |>
  features(Trips, feat_stl) |>
  filter(
    seasonal_strength_year == max(seasonal_strength_year)
  ) |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>    # c("State", "Region", "Purpose") is the key :)       ||||||||||    "all", the default, returns every match detected in y. This is the same behavior as SQL.
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() + 
  facet_grid(vars(State, Region, Purpose))

################################################################################
#
# # ?vars
# # vars() is a quoting function that takes inputs to be evaluated in the context of a dataset. These inputs can be:
# 
# variable names
# 
# complex expressions
# 
# In both cases, the results (the vectors that the variable represents or the results of the expressions) are used to form faceting groups.
# 
################################################################################

# # The feat_stl() function returns several more features other than those discussed above.# 
# seasonal_peak_year indicates the timing of the peaks — which month or quarter contains the largest seasonal component. This tells us something about the nature of the seasonality. In the Australian tourism data, if Quarter 3 is the peak seasonal period, then people are travelling to the region in winter, whereas a peak in Quarter 1 suggests that the region is more popular in summer.
# seasonal_trough_year indicates the timing of the troughs — which month or quarter contains the smallest seasonal component.
# spikiness measures the prevalence of spikes in the remainder component  # Rt of the STL decomposition. It is the variance of the leave-one-out variances of Rt.
# linearity measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.
# curvature measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.
# stl_e_acf1 is the first autocorrelation coefficient of the remainder series.
# stl_e_acf10 is the sum of squares of the first ten autocorrelation coefficients of the remainder series.

# ?left_join

```


```{r More Features in feasts}
# 
# The remaining features in the feasts package, not previously discussed, are listed here for reference. The details of some of them are discussed later in the book.
# 
# coef_hurst will calculate the Hurst coefficient of a time series which is a measure of “long memory”. A series with long memory will have significant autocorrelations for many lags.
# feat_spectral will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast. A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0. A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
# box_pierce gives the Box-Pierce statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
# ljung_box gives the Ljung-Box statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
# The  
# k
#  th partial autocorrelation measures the relationship between observations  
# k
#   periods apart after removing the effects of observations between them. So the first partial autocorrelation ( 
# k
# =
# 1
#  ) is identical to the first autocorrelation, because there is nothing between consecutive observations to remove. Partial autocorrelations are discussed in Section 9.5. The feat_pacf function contains several features involving partial autocorrelations including the sum of squares of the first five partial autocorrelations for the original series, the first-differenced series and the second-differenced series. For seasonal data, it also includes the partial autocorrelation at the first seasonal lag.
# unitroot_kpss gives the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) statistic for testing if a series is stationary, and the corresponding p-value. This test is discussed in Section 9.1.
# unitroot_pp gives the Phillips-Perron statistic for testing if a series is non-stationary, and the corresponding p-value.
# unitroot_ndiffs gives the number of differences required to lead to a stationary series based on the KPSS test. This is discussed in Section 9.1
# unitroot_nsdiffs gives the number of seasonal differences required to make a series stationary. This is discussed in Section 9.1.
# var_tiled_mean gives the variances of the “tiled means” (i.e., the means of consecutive non-overlapping blocks of observations). The default tile length is either 10 (for non-seasonal data) or the length of the seasonal period. This is sometimes called the “stability” feature.
# var_tiled_var gives the variances of the “tiled variances” (i.e., the variances of consecutive non-overlapping blocks of observations). This is sometimes called the “lumpiness” feature.
# shift_level_max finds the largest mean shift between two consecutive sliding windows of the time series. This is useful for finding sudden jumps or drops in a time series.
# shift_level_index gives the index at which the largest mean shift occurs.
# shift_var_max finds the largest variance shift between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the volatility of a time series.
# shift_var_index gives the index at which the largest variance shift occurs.
# shift_kl_max finds the largest distributional shift (based on the Kulback-Leibler divergence) between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the distribution of a time series.
# shift_kl_index gives the index at which the largest KL shift occurs.
# n_crossing_points computes the number of times a time series crosses the median.
# longest_flat_spot computes the number of sections of the data where the series is relatively unchanging.
# stat_arch_lm returns the statistic based on the Lagrange Multiplier (LM) test of Engle (1982) for autoregressive conditional heteroscedasticity (ARCH).
# guerrero computes the optimal  
# λ
#   value for a Box-Cox transformation using the Guerrero method (discussed in Section 3.1).


```


```{r Computing all features :)}
################################################################################
################################################################################
###############################                  ###############################
###############################     feasts       ###############################
###############################                  ###############################
################################################################################
################################################################################

tourism_features <- tourism |>
  features(Trips, feature_set(pkgs = "feasts"))

tourism_features

```


```{r}
# all features that involve seasonality, along with the Purpose variable.

library(glue)
x11()

tourism_features |>
  select_at(vars(contains("season"), Purpose)) |>                      # features with season
  mutate(
    seasonal_peak_year = seasonal_peak_year +
      4*(seasonal_peak_year==0),                                  # A conditional check for 0 and then modify it to 4 (without                                                                      using ifelse)
    seasonal_trough_year = seasonal_trough_year +
      4*(seasonal_trough_year==0),
    
    seasonal_peak_year = glue("Q{seasonal_peak_year}"),           # This wraps the numeric value with a "Q" prefix, Q1
    
    seasonal_trough_year = glue("Q{seasonal_trough_year}"),       # wraps numeric value
  ) |>
  GGally::ggpairs(mapping = aes(colour = Purpose))
x11()
```


```{r}

library(broom)

pcs <- tourism_features |>

  select(-State, -Region, -Purpose) |>
  
  prcomp(scale = TRUE) |>                                 # PCA analysis, scaling standardies the features mean=0, stdev=1 
  
  augment(tourism_features)     # augment() from broom adds fitted values/PCA scores back to original data (tourism_features).


pcs |>
  
  ggplot(
    aes(
      x = .fittedPC1,
      y = .fittedPC2,
      col = Purpose)
    ) +
  geom_point() +
  theme(aspect.ratio = 1)

```


```{r}

outliers <- pcs |>
  filter(.fittedPC1 > 10) |>                                # 10 is arbitraray choosen lets say from plot
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers

#> # A tibble: 4 × 5
#>   Region                 State             Purpose  .fittedPC1 .fittedPC2
#>   <chr>                  <chr>             <chr>         <dbl>      <dbl>
#> 1 Australia's North West Western Australia Business       13.4    -11.3  
#> 2 Australia's South West Western Australia Holiday        10.9      0.880
#> 3 Melbourne              Victoria          Holiday        12.3    -10.4  
#> 4 South Coast            New South Wales   Holiday        11.9      9.42
#> 

outliers |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>
  mutate(Series = glue("{State}", "{Region}", "{Purpose}", .sep = "\n\n")) |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = "free") +
  labs(title = "Outlying time series in PC space")

```
### Forecaster's Toolbox

To illustrate the process, we will fit linear trend models to national GDP data stored in global_economy.

```{r tidy forecasting window}

# Data preparation (tidy)

tsibbledata::global_economy

gdppc <- tsibbledata::global_economy |>
  dplyr::mutate(GDP_per_capita = GDP/Population)

gdppc

# Plotting for visaulization (1 country here) sweden

gdppc |>
  filter(Country == "Sweden") |>
  autoplot(GDP_per_capita) +
  labs(y="$US", title = "GDP per capita of Sweden")
  
###################################################################################
# Model specification

# a lot of available models to choose from 
# Models in fable are specified using model functions, which each use a formula (y ~ x) interface. 

########################## TSLM(GDP_per_capita ~ trend()) #########################

# In this case the model function is TSLM() (time series linear model), the response variable is GDP_per_capita and it is being modelled using trend() (a “special” function specifying a linear trend when it is used within TSLM()). We will be taking a closer look at how each model can be specified in their respective sections.
# 
###################################################################################

# Training model estiamting :)

fit <- gdppc |>
  model(trend_model = TSLM(GDP_per_capita ~ trend()))

fit


# Model performance (evaluation)

# for that we will use diagnostics (later)

# Forecasts
#
######### produce the forecasts using forecast()

fit |> forecast(h = "3 years")


# plotting forecasts

fit |>
  forecast(h = "3 years") |>
  filter(Country == "Sweden") |>
  autoplot(gdppc) +
  labs(y = "$US", title = "GDP per capita for Sweden")


```


```{r forecasting methods}

# just model syntaxes here

# 4 simple benchmarking methods :)
# 1. Mean Method
# 2. naive Forecast
# 3. Seasonal Naive
# 4. Drift 

aus_production

# going for bricks production data

bricks <- aus_production|>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Bricks)
bricks

# going for beer data too

beer <- aus_production |>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Beer)
beer

# 1. Mean Method
bricks |> fabletools::model(MEAN(Bricks))
beer |> fabletools::model(MEAN(Beer))


# 2. naive Forecast
bricks |> model(NAIVE(Bricks))
beer |> model(NAIVE(Beer))

bricks |> model(RW(Bricks))

# a naïve forecast is optimal when data follow a random walk (Section 9.1), 
# these are also called random walk forecasts and the RW() function can be used instead of NAIVE

# 3. Seasonal Naive
bricks |> model(SNAIVE(Bricks ~ lag("year")))
# The lag() function is optional here as bricks is quarterly data and so a seasonal naïve method will need a one-year lag.


# 4. Drift 
bricks |> model(RW(Bricks ~ drift()))


```


```{r}
# fitting proper models with forecasts (for Beer)

# Set training data from 1970 to 2006
train <- aus_production |>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Beer)

# Fit the Model
beer_fit <- train |>
  model(
    Mean = MEAN(Beer),
    NAIVE = NAIVE(Beer),
    SNAIVE = SNAIVE(Beer),            # The lag special is used to specify the lag order for the                                          random walk process. If left out, this special will                                               automatically be included.
    DRIFT = RW(Beer ~ drift())
    
  )

# generating forecasts for 14 quarters

beer_fc <- beer_fit |> 
  forecast(h=30)

# Plotting forecasts
beer_fc |> 
  autoplot(train, level=NULL) +              # level is for confidence interval, can accpt vector (def. 85 - 90%)
  autolayer(
    filter_index(aus_production, "2004 Q1" ~ .),
    colour = "Black"
  ) + 
  labs(
    y = "Mega L",
    title = "Forecasts for Quarterly Beer Production"
  ) +
  guides(
    colour = guide_legend(title="Forecast")
  )



```


```{r Google’s daily closing stock price}

# Re-index based on trading days

# ?gafa_stock # Historical stock prices from 2014-2018 for Google, Amazon, Facebook and Apple ($USD).

google_stock <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) >= 2015)|>
  mutate(day = dplyr::row_number()) |>
  update_tsibble(index = day, regular = TRUE)


# Filter the year of interest
google_2015 <- google_stock |> filter(year(Date) == 2015)


# Fit the models
google_fit <- google_2015 |>
  model(
    Mean = MEAN(Close),
    Naive = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )


# Produce forecasts for the trading days in January 2016

google_jan_2016 <- google_stock |>
  filter(yearmonth(Date) == yearmonth("2016 Jan"))       # for january forecasts only

google_fc <- google_fit |>
  forecast(new_data = google_jan_2016)

################################################################################
# Note :::::::::::::::
# 
# Forecast using new_data: 
# This is used when you want to forecast specific future time points, especially if:
# Your time series is irregular (like stock market data — only trading days).
# 
# You want forecasts for custom dates (not just the next h steps).
############
#             forecast(model, new_data = some_data)
############
# "some_data" must be a "data frame" or "tsibble" that includes the future time points you want forecasts for.
# 
# You manually provide the time index (like dates) for which you want forecasts.
# se when time is irregular or you want to forecast specific dates.
#
################################################################################


# Plot the forecasts
google_fc |>
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, colour = "black") +
  labs(y = "$US",
       title = "Google daily closing stock prices",
       subtitle = "(Jan 2015 - Jan 2016)") +
  guides(colour = guide_legend(title = "Forecast"))


```


```{r fitted values and residuals}

# Augment - apply augment() to this object to compute the fitted values and residuals for all models.

augment(beer_fit)

################################################################################
#
# There are three new columns added to the original data:
#
# .fitted contains the fitted values;
# .resid contains the residuals;
# .innov contains the “innovation residuals” which, in this case, are identical to the regular residuals.
# 
################################################################################

# #################################################################################
# # Note: 
# 
# Residuals are useful in checking whether a model has adequately captured the information in the data. For this purpose, we use innovation residuals.
# 
# If patterns are observable in the innovation residuals, the model can probably be improved. We will look at some tools for exploring patterns in residuals in the next section.
# 
# #################################################################################

```

##### Residual diagnostics

```{r}
# good forecasting
### uncorrelated inoovation residuals
### zero mean of innov. residuals

# Further good/ useful to have ->
# constant variance on innov resid 
# innov. resid. normally distributed
# 

# Forecasting Google daily closing stock prices
autoplot(google_2015, Close) +
  labs(y = "$US",
       title = "Google daily closing stock prices in 2015")

# The following graph shows the Google daily closing stock price for trading days during 2015. The large jump corresponds to 17 July 2015 when the price jumped 16% due to unexpectedly strong second quarter results



aug <- google_2015 |>
  fabletools::model(NAIVE(Close)) |>
  augment()                           # to get residual data
autoplot(aug, .innov) +
  labs(y = "$US",
       title = "Residuals from the naïve method")


# hostogram

aug |>
  ggplot(aes(x = .innov)) +
  geom_histogram() +
  labs(title = "Histogram of residuals")

# ACF
aug |>
  feasts::ACF(.innov) |>
  autoplot() +
  labs(title = "Residuals from the naïve method")




```
Easy way by single function -> gg_tsresiduals()

```{r}

google_2015 |>
  model(NAIVE(Close)) |>
  gg_tsresiduals()

```


```{r Portmanteau tests for autocorrelation}

# A test for a group of autocorrelations is called a portmanteau test

# Box-Pierce
aug |> features(.innov, box_pierce, lag = 10)

# Ljung Box
aug |> features(.innov, ljung_box, lag = 10)

# 
# p-value > 0.05 -> For both  
# Q and Q∗, the results are not significant (i.e., the  p-values are relatively large).
# Thus, we can conclude that the residuals are not distinguishable from a white noise series

# Box-Pierce 
# 
# NUll : Ho : a time series is white noise
#        so, if p-value<0.05 -> Reject NUll i.e. it is white noise...meaning It is not White Noise
#       Now, for p-value > 0.05 -> The series is Almost White Noise (NUll cannot be rejected)
# 
```


```{r}
# Drift Method

# An alternative simple approach that may be appropriate for forecasting the Google daily closing stock price is the drift method. The tidy() function shows the one estimated parameter, the drift coefficient, measuring the average daily change observed in the historical data.


fit <- google_2015 |> model(RW(Close ~ drift()))

augment(fit) |> features(.innov, ljung_box, lag=10)

# As with the naïve method, the residuals from the drift method are indistinguishable from a white noise series.

```


```{r Forecast distributions + Prediction Intervals}

# #####################################################################################################
# 
#  hilo():    The hilo() function converts the forecast distributions into intervals.
#             By default, 80% and 95% prediction intervals are returned, although 
#             other options are possible via the level argument
# 
# #####################################################################################################

google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  hilo()

# No level
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015, level=NULL) +                           # default level is 80 and 95%
  labs(title="Google daily closing stock price", y="$US" )

# level by default
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015) +
  labs(title="Google daily closing stock price", y="$US" )

# passing levels
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015, level=c(85, 90, 95, 99)) +
  labs(title="Google daily closing stock price", y="$US" )


```
#### Prediction Interval fro mBootstrapped Samples

When a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance

```{r Prediction Interval fro mBootstrapped Samples}

fit <- google_2015 |>
  model(NAIVE(Close))

simulate <- fit |> generate(h = 30, times = 5, bootstrap = TRUE)
simulate

# Here we have generated five possible sample paths for the next 30 trading days. The .rep variable provides a new key for the tsibble.

# we generated 5 sample paths so have index of values for each path too

google_2015 |>
  ggplot(aes(x = day)) +
  geom_line(aes(y = Close)) +
  geom_line(
    data = simulate,
    aes(y = .sim, colour = as.factor(.rep))) +
  labs(title="Google daily closing stock price", y="$US" ) +
  guides(colour = "none")


```
#### Shortcut for bootstrap (in forecast())

```{r}

fc <- fit |> forecast(h = 30, bootstrap = TRUE)
fc

# but now we have 5000 sample paths :)

autoplot(fc, google_2015) +
  labs(title="Google daily closing stock price", y="$US" )

# note how smart autoplot is here :)

```


```{r}

fc1000 <- 
  google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10, bootstrap = TRUE, times = 1000) |>
  hilo()

str(fc1000$.mean)

# 
# autoplot(fc1000, google_2015)+
#   labs(title = "Google daily stock closing price", y = "$US")
# Plot won't work with hilo here ..........
# 
# autoplot() tries to plot .mean (which is length 10) but also tries to handle those new hilo interval columns.
# 
# Since those intervals are stored as list objects, ggplot2 fails when mapping aesthetics because it expects numeric vectors, not lists.
# 
# This leads to the error about aesthetics length mismatch.

```


```{r Forecasting with transformations}

prices # commodity prices

fc <- prices |>
  filter(!is.na(eggs)) |>                  # removing NA
  model(RW(log(eggs) ~ drift())) |>        # log transfrom of prices and a drift model
  forecast(h = 50) |>                      # forecast for 50 horizons
  mutate(.median = median(eggs))           # new column with median value of eggs from the distribution in new fc

fc

fc |>
  autoplot(prices |> filter(!is.na(eggs)), level = c(80, 90, 95)) +
  geom_line(
    aes(y = .median),
    data = fc,
    linetype = 2,
    col = "blue") +
  labs(title = "Annual egg prices",
       y = "$US (in cents adjusted for inflation) ")


```


```{r Forecasting with decomposition}

# example - Employment in the US retail sector

us_employment

us_employment |>
  filter(
    year(Month) >= 1990, Title =="Retail Trade") -> us_retail_employment
  
dcmp <- us_retail_employment |>
  model(STL(Employed ~ trend(window=7), robust = TRUE)) |>
  components() |>
  select(-.model)
# |>
#   autoplot(trend)

dcmp

##########################################################################################
##########################################################################################
##########################################################################################

dcmp |>
  model(NAIVE(season_adjust)) |>
  forecast() |>
  autoplot(dcmp) +
  labs(y = "Number of people",
       title = "US retail employment")


#########################################################################################
#########################################################################################
#########################################################################################

#                              Easy Version                                             #
                      
#                          decomposition_model()                                        #

# allows  computation of forecasts via any additive decomposition, 
# using other model functions to forecast each of the decomposition’s components.
# 
# Seasonal components of the model will be forecast automatically using SNAIVE() if
# a different model isn’t specified.
# The function will also do the reseasonalising for, ensuring
# that the resulting forecasts of the original data are obtained. 

fit_dcmp <- us_retail_employment |>
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window=7), robust = TRUE),
    NAIVE(season_adjust)
  ))

fit_dcmp |> 
  forecast() |>                     # by default uses the same period of forecast as the seasonal period
  autoplot(us_retail_employment) + 
  labs(
    y = "Number of people",
    title = "US retail Employment"
  )

# If your data is monthly, it forecasts the next 12 periods (i.e., 12 months).
# If it's quarterly, then 4 periods (1 year).
# If it's weekly, then 52 periods.

fit_dcmp |> gg_tsresiduals()



#########################################################################################
#########################################################################################
#########################################################################################
#########################################################################################
```


```{r Evaluating point forecast accuracy}
# Subset of time series 

aus_production |> 
  filter(year(Quarter) >= 1995)


aus_production |>
  filter_index("1995 Q1" ~ .) # all data from 1995 Q1

# Slice -------------------- 
# allows the use of indices to choose a subset from each group. For example,
aus_production |>
  slice((n()-19):n())       # extracts the last 20 observations (5 years).
# or slice(n()-19:0)
# means last 20 obs

# slicing with groups
aus_retail |>
  group_by(State, Industry) |>
  slice(1:12)                              # selects the first 12 rows from each group.


```


```{r Forecast errors}

recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)

beer_train <- recent_production |>
  filter(year(Quarter) <= 2007)

beer_fit <- beer_train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )

beer_fc <- beer_fit |>
  forecast(h = 10)

beer_fc |>
  autoplot(
    aus_production |> filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))

```

Accuracy

```{r}

# The accuracy() function will automatically extract the relevant periods from
# the data (recent_production in this example) to match the forecasts when 
# computing the various accuracy measures.

accuracy(beer_fc, recent_production)



```


```{r Non-Seasonal Example}

google_fit <- google_2015 |>
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = RW(Close ~ drift())
  )

google_fc <- google_fit |>
  forecast(google_jan_2016)

google_fc |>
  autoplot(bind_rows(google_2015, google_jan_2016),
    level = NULL) +
  labs(y = "$US",
       title = "Google closing stock prices from Jan 2015") +
  guides(colour = guide_legend(title = "Forecast"))

accuracy(google_fc, google_stock)



```


```{r Evaluating distributional forecast accuracy}

# Quantile scores

google_fc # forecasted data df

google_fc |>
  filter(.model == "Naïve") |>
  autoplot(bind_rows(google_2015, google_jan_2016), level=80)+
  labs(y = "$US",
       title = "Google closing stock prices")


google_fc |>
  filter(.model == "Naïve", Date == "2016-01-04") |>
  accuracy(google_stock, list(qs=quantile_score), probs=0.10)



# Winkler Score

google_fc |>
  filter(.model == "Naïve", Date == "2016-01-04") |>
  accuracy(google_stock,
    list(winkler = winkler_score), level = 80)


# COnt ranked probability
google_fc |>
  accuracy(google_stock, list(crps = CRPS))


#####
google_fc |>
  accuracy(google_stock, list(skill = skill_score(CRPS)))

```


```{r Time series cross-validation}
################################################################################
################################################################################
################################################################################
                          # 
                          # Growing Window
                          # Sliding Window ?
                          # Rolling Window ?
                          # 
                          # 
################################################################################
################################################################################
################################################################################


# Growing window function used for time series cross-validation #

# The stretch_tsibble() function is used to create many training sets. 
# In this example, we start with a training set of length .init=3, and 
# increase the size of successive training sets by .step=1


google_2015_tr <- google_2015 |>

  tsibble::stretch_tsibble(.init = 3, .step = 1) |>        # Growing window function used for time series cross-validation
  
  relocate(Date, Symbol, .id)                     # reorders columns, putting Date, Symbol, and .id first in the                                                       output for easier reading
google_2015_tr

# .id with sequence numbers tell which are used in training iterations... so technically it is making redundant copies of data here..



# other functions ::::::
# slide_tsibble() :::::: - Perform Sliding Window
# tile_tsibble()  :::::: - Perform Tilling Window


# slide_tsibble(.x, .size = 1, .step = 1, .id = ".id")
# Arguments
# .x	 -  # A tsibble.
# 
# .size	- #  A positive integer for window size.
# 
# .step	- # A positive integer for calculating at every specified step instead of every single step.
# How far to move forward each time.
# 
# .id	  - # A character naming the new column .id containing the partition.
# Name of the new column that stores which window each row came from.
#

#  slide_tsibble(), tile_tsibble(), and stretch_tsibble() provide fast and shorthand for rolling over a tsibble by observations. That said, if the supplied tsibble has time gaps, these rolling helpers will ignore those gaps and proceed.
# They are useful for preparing the tsibble for time series cross validation. They all return a tsibble including a new column .id as part of the key. The output dimension will increase considerably with slide_tsibble() and stretch_tsibble(), which is likely to run out of memory when the data is large.


### AN Additional Example ###

# harvest <- tsibble(
#   year = rep(2010:2012, 2),
#   fruit = rep(c("kiwi", "cherry"), each = 3),
#   kilo = sample(1:10, size = 6),
#   key = fruit, index = year
# )
# harvest %>%
#   slide_tsibble(.size = 2)


# Notice the use of key... helps generate separate time series for each key values



#####################################################################################
###                          
###                          Tile Tsibble
###                          
###             Partitions your time series into 
###             non-overlapping tiles (chunks) of
###             size .size, within each key group.
###          
###          it jumps ahead by .size rows each time
###         (i.e., it skips rows and avoids overlaps).     
###             
###                          
#####################################################################################

# Usage
# tile_tsibble(.x, .size = 1, .id = ".id")
# Arguments
# .x	
# A tsibble.
# 
# .size	
# A positive integer for window size.
# 
# .id	
# A character naming the new column .id containing the partition.

# 
# tile_tsibble(.size = 2)
# 
# This splits each fruit's time series into non-overlapping chunks (tiles) of 2 rows:
# 
# For kiwi:
# 
# Tile 1: 2010, 2011
# 
# Tile 2: 2012
# 
# For cherry:
# 
# Tile 1: 2010, 2011
# 
# Tile 2: 2012


```


```{r Accuracy function}
#  The accuracy() function can be used to evaluate the forecast accuracy across the training sets.

google_2015_tr

# TSCV accuracy
google_2015_tr |>
  model(RW(Close ~ drift())) |>              # drift-model
  forecast(h = 1) |>                         # 1-step ahead forecast
  accuracy(google_2015)     # (Automaticlly takes care of Keys-Match_data & Calc. Accuracy)

#?accuracy

# Training set accuracy
google_2015 |>
  model(RW(Close ~ drift())) |>
  accuracy()

####################################################################################################
####################################################################################################

# As expected, the accuracy measures from the residuals are smaller, as the corresponding “forecasts” are based on a model fitted to the entire data set, rather than being true forecasts.
 
# A good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation.

####################################################################################################
####################################################################################################

```

###### Forecast horizon accuracy with cross-validation
```{r forecasting performance of 1- to 8-step-ahead drift forecasts}

google_2015

google_2015_tr <- google_2015 |>
  tsibble::stretch_tsibble(
    .init = 3,
    .step = 1
  )

google_2015_tr

fc <- google_2015_tr |>
  model(
    RW(Close ~ drift())
  ) |>
  forecast(h=8) |>
  group_by(.id) |>
  mutate(h = row_number()) |>
  ungroup() |>
  as_fable(response = "Close", distribution = Close)

fc

# as_fable() is used to construct a fable (forecast table) from a tsibble and forecast distribution


# This helps you see how forecast error increases (or decreases) over longer forecast horizons.
# Accuracy calculation over the different horizons

fc |>
  accuracy(google_2015, by = c("h", ".model")) |>
  ggplot(aes(x = h, y = RMSE)) +
  geom_point()


```

## Time Series Regression Models
#### The linear model

```{r}
# US consumption expenditure

us_change

colnames(us_change)
str(us_change)

us_change |>
  tidyr::pivot_longer(
    c(Consumption, Income),
    names_to="Series") |>
  autoplot(value) +
  labs(y = "% change")

# model

us_change |>
  ggplot(
    aes(x = Income,
        y = Consumption)) +
  
  labs(y = "Consumption (quarterly % change)",
       x = "Income (quarterly % change)") +
  
  geom_point() +
  
  geom_smooth(method = "lm", se = FALSE)           # lineary regression line


```
##### using TSLM

```{r using TSLM}

us_change |>
  model(TSLM(Consumption ~ Income)) |>
  report()                                     

```

#### Multiple linear regression

```{r}

us_change

us_change |>
  select(-Consumption, -Income)

us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter)    # same as pivot_longer(cols = !Quarter) | Keeps Quarter col, and pivot everything else

us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter) |>
  ggplot(aes(Quarter, value, colour = name))

us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter) |>
  ggplot(aes(Quarter, value, colour = name)) +
  geom_line()

us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter) |>
  ggplot(aes(Quarter, value, colour = name)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y")

us_change |>
  select(-Consumption, -Income) |>              
  pivot_longer(-Quarter) |>                       
  ggplot(aes(Quarter, value, colour = name)) +  # aesthetic mappings, x,y,color, shape, size, fill, group etc.
  geom_line() +                                 #
  facet_grid(name ~ ., scales = "free_y") +
  guides(colour = "none")                       # Just drops the legend (As not needed now as we have diff. facets)

us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter) |>
  ggplot(aes(Quarter, value, colour = name)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  guides(colour = "none") +
  labs(y="% change")                              # just labelling the y axis

```
##### Scatterplots and Correlation Plots

```{r}

us_change |>
  GGally::ggpairs(columns = 2:6)

```

Assumptions
First, we assume that the model is "a reasonable approximation to reality";
that is, the relationship between the forecast variable and the predictor variables satisfies this linear equation.

Second, we make the following assumptions about the errors  (ε1,…,εT): 
<!-- they have mean zero; otherwise the forecasts will be systematically biased.  -->
<!-- they are not autocorrelated; otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited. -->
<!-- they are unrelated to the predictor variables; otherwise there would be more information that should be included in the systematic part of the model. -->

It is also useful to have the errors being normally distributed with a constant variance  σ2  in order to easily produce prediction intervals.

Another important assumption in the linear regression model is that each predictor  x  is not a random variable. 

With observational data (including most data in business and economics), it is not possible to control the value of  x , we simply observe it. Hence we make this an assumption.

##### Least squares estimation

```{r Least squares estimation}

# US consumption expenditure

us_change

fit_consMR <- us_change |>
  model(tslm = TSLM(Consumption ~ Income + Production +
                                    Unemployment + Savings))           # multi-linear ergression
fit_consMR

report(fit_consMR)          # model report



```
###### augment() function for fitted values

```{r Fitted values}

# Fitted values

augment(fit_consMR)       # gives, .model (name) .fitted, .resid, .innov REsiduals

augment(fit_consMR) |>
  ggplot(
    aes(x=Quarter)
  ) +
  geom_line(aes(y=Consumption, colour = "Data")) + # "Data" manual legend trick (scale_colour_manual is follwed)
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "% change",
    title = "Percent change in US consumption expenditure"
  ) +
  scale_colour_manual(values=c(Data="black",Fitted="#D55E00")) + 
  guides(colour = guide_legend(title = NULL))                    # removing legend title ? else it will be colour

  



```


```{r}
# with abline fitted on graph (i.e. y and y_hat)

augment(fit_consMR)

augment(fit_consMR) |>
  
  ggplot(aes(x = Consumption, y = .fitted)) +
  
  geom_point() +
  
  labs(
    y = "Fitted (predicted values)",
    x = "Data (actual values)",
    title = "Percent change in US consumption expenditure"
  ) +
  
  geom_abline(intercept = 0, slope = 1)

```
##### Evaluating the regression model

Residual plot
ACF of residuals
Histogram of residuals

```{r}

fit_consMR |> gg_tsresiduals()


```
The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate.

The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals.

The autocorrelation plot shows a significant spike at lag 7, and a significant Ljung-Box test at the 5% level.

```{r}

augment(fit_consMR) |>
  features(.innov, ljung_box, lag = 10)

```
Thus, Residuals are not a white noise, and there is still info left in residuals. (For getting that info..CHapter 10) -- Dynamic Regression Techniques

###### Residual plots against predictors

```{r}
# Residual plots against predictors (all)

us_change
fit_consMR
residuals(fit_consMR)
str(fit_consMR)

fit_consMR$tslm

residuals(fit_consMR)

us_change |>
  
  left_join(residuals(fit_consMR), by = "Quarter") |>
  
  pivot_longer(Income:Unemployment,
               names_to = "regressor", values_to = "x") |>
  
  ggplot(aes(x = x, y = .resid)) +
  
  geom_point() +
  
  facet_grid(. ~ regressor, scales = "free_x") +
  
#  facet_wrap(. ~ regressor, scales = "free_x") +              # facet_wrap(rows ~ cols)
  
  labs(y = "Residuals", x = "")


us_change |>
  
  left_join(residuals(fit_consMR), by = "Quarter") |>
  
  pivot_longer(Income:Unemployment,
               names_to = "regressor", values_to = "x") |>
  
  ggplot(aes(x = x, y = .resid)) +
  
  geom_point() +
  
#  facet_grid(. ~ regressor, scales = "free_x") +
  
  facet_wrap(. ~ regressor, scales = "free_x") +              # facet_wrap(rows ~ cols) #automatic wrapping
  
  labs(y = "Residuals", x = "")

```
###### Residual plots against fitted values

```{r}
# Residual plots against fitted values

fit_consMR
augment(fit_consMR)

augment(fit_consMR) |>
  
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() + labs(x = "Fitted", y = "Residuals")

```
###### Outliers and influential observations

```{r Outliers and influential observations}
# Outliers and influential observations
# Observations that have a large influence on the estimated coefficients of a regression model are called influential observations

```

###### Spurious regression

```{r Spurious regression}
# Spurious regression

# More often than not, time series data are “non-stationary”; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance.
# here we need to address the effect that non-stationary data can have on regression models.


# Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future.

aus_airpassengers         # Year | Passengers

guinea_rice               # Rice Production Year | Production

fit <- aus_airpassengers |>

  filter(Year <= 2011) |>
  
  left_join(guinea_rice, by = "Year") |>
  
  model(TSLM(Passengers ~ Production))             # Linear model TSLM

report(fit)

fit |> gg_tsresiduals()



```
Regressing non-stationary time series can lead to spurious regressions. The output of regressing Australian air passengers on rice production in Guinea is shown in Figure 7.13. High R2  and high residual autocorrelation can be signs of spurious regression.

The innovation residuals are not white noise...so not a model we are looking for.

###### Some useful predictors

```{r}
# Some useful predictors

# just australian beer production data and plot 
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
recent_production |>
  autoplot(Beer) +
  labs(y = "Megalitres",
       title = "Australian quarterly beer production")

# forecasting value of future beer production. We can model this data using a regression model with a linear trend and quarterly dummy variables,

# i.e. Forecast ~ Linear Trend (regression) + Quarter Dummies

fit_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + season()))           # just added seaspn (takes in season automatically)

# Note that trend() and season() are not standard functions; they are “special” functions that work within the TSLM() model formulae.

report(fit_beer)

# Fitted values

augment(fit_beer) |>
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  scale_colour_manual(
    values = c(Data = "black", Fitted = "#D55E00")
  ) +
  labs(y = "Megalitres",
       title = "Australian quarterly beer production") +
  guides(colour = guide_legend(title = "Series"))


# actual vs fitted
augment(fit_beer) |>
  ggplot(aes(x = Beer, y = .fitted,
             colour = factor(quarter(Quarter)))) +
  geom_point() +
  labs(y = "Fitted", x = "Actual values",
       title = "Australian quarterly beer production") +
  geom_abline(intercept = 0, slope = 1) +
  guides(colour = guide_legend(title = "Quarter"))

```

###### Intervention variables -->
<!-- # necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect. -->
<!-- #  -->
<!-- # When the effect lasts only for one period, we use a “spike” variable. This is a dummy variable that takes value one in the period of the intervention and zero elsewhere. A spike variable is equivalent to a dummy variable for handling an outlier. -->
<!-- #  -->
<!-- # Other interventions have an immediate and permanent effect. If an intervention causes a level shift (i.e., the value of the series changes suddenly and permanently from the time of intervention), then we use a “step” variable. A step variable takes value zero before the intervention and one from the time of intervention onward. -->
<!-- #  -->
<!-- # Another form of permanent effect is a change of slope. Here the intervention is handled using a piecewise linear trend; a trend that bends at the time of intervention and hence is nonlinear. We will discuss this in Section 7.7. -->

Trading days
Distributed lags
Easter

###### Fourier series
These Fourier terms are produced using the fourier() function. For example, the Australian beer data can be modelled like this.

```{r}

fourier_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + fourier(K = 2)))

report(fourier_beer)

################################                                              ################################
#
# The K argument to fourier() specifies how many pairs of sin and cos terms to include. The maximum allowed is  
#                                         K = m / 2  where m is the seasonal period
################################                                              ################################

```
######  Selecting predictors

.................... adj_r_squared | CV | AIC | AICc | BIC ....................

.................... Best subset regression
.................... Stepwise regression
.............................. Backward
.............................. Forward
.............................. Hybrid 
....................Beware of inference after selecting the predictors (we are forecasting not inferencing :)
....................

....................       using glance() function         ....................

```{r}

fit_consMR
residuals(fit_consMR)

glance(fit_consMR) |>
  select(adj_r_squared, CV, AIC, AICc, BIC)


```

###### Forecasting with regression

```{r Forecasting with regression}
# Ex-ante versus ex-post forecasts

aus_production

recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
recent_production

fit_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + season()))      # trend (for time variable) + season
fit_beer

fc_beer <- forecast(fit_beer)                 # forecast for default horizon


fc_beer |>
  autoplot(recent_production) +
  labs(
    title = "Forecasts of beer production using regression",
    y = "megalitres"
  )


```
###### Scenario based forecasting

```{r Scenario based forecasting}

us_change

fit_consBest <- us_change |>
  model(
    lm = TSLM(Consumption ~ Income + Savings + Unemployment)
  )

fit_consBest

future_scenarios <- scenarios(              # creating new data for which to forecast

  Increase = new_data(us_change, 4) |>
    mutate(Income=1, Savings=0.5, Unemployment=0),
                                                    # new_data(your_tsibble, n)
                                                    # your_tsibble = your original data (must be a tsibble with a                                                      # time index)
                                                    # n = number of future time points you want to create
  Decrease = new_data(us_change, 4) |>
    mutate(Income=-1, Savings=-0.5, Unemployment=0),
  names_to = "Scenario")
future_scenarios

fc <- forecast(fit_consBest, new_data = future_scenarios)
fc

us_change |>
  autoplot(Consumption) +
  autolayer(fc) +
  labs(title = "US consumption", y = "% change")

```
###### Building a predictive regression model
####### Prediction intervals


```{r}

us_change

fit_cons <- us_change |>
  model(TSLM(Consumption ~ Income))
fit_cons

new_cons <- scenarios(
  "Average increase" = new_data(us_change, 4) |>
    mutate(Income = mean(us_change$Income)),
  "Extreme increase" = new_data(us_change, 4) |>
    mutate(Income = 12),
  names_to = "Scenario"
)

new_cons

fcast <- forecast(fit_cons, new_cons)
fcast


us_change |>
  autoplot(Consumption) +
  autolayer(fcast) +
  labs(title = "US consumption", y = "% change")

```

###### Nonlinear regression

```{r}
# Non-linear trend

boston_marathon

boston_men <- boston_marathon |>
  filter(Year >= 1924) |>
  filter(Event == "Men's open division") |>
  mutate(Minutes = as.numeric(Time)/60)

boston_men

##################################################
##################################################
##################################################
##################################################

# # piecewise linear - One of the simplest specifications is to make  f   piecewise linear. That is, we introduce points where the slope of  f   can change. These points are called knots. This can be achieved by letting  
# x1=x  and introducing variable  x2  such that

# The notation  (x−c)+  means the value  x−c  if it is positive and 0 otherwise. This forces the slope to bend at point  c. Additional bends can be included in the relationship by adding further variables of the above form.



fit_trends <- boston_men |>
  model(
    linear = TSLM(Minutes ~ trend()),                                 # linear Model
    exponential = TSLM(log(Minutes) ~ trend()),                       # Exponential | Log model
    piecewise = TSLM(Minutes ~ trend(knots = c(1950, 1980)))          # piecewise linear
  )
fit_trends

fc_trends <- fit_trends |> forecast(h = 10)                           # forecasting 10 horizons
fc_trends

boston_men |>
  autoplot(Minutes) +
  geom_line(data = fitted(fit_trends),
            aes(y = .fitted, colour = .model)) +
  autolayer(fc_trends, alpha = 0.5, level = 95) +
  labs(y = "Minutes",
       title = "Boston marathon winning times")



```
The best forecasts appear to come from the piecewise linear trend.

###### Correlation, causation and forecasting

```{r Correlation, causation and forecasting}

# Correlation, causation and forecasting

# Correlation is not causation

# Forecasting with correlated predictors

# Multicollinearity and forecasting


####################################################################################
#                                                                               ####
# Matrix formulation                                                            ####
# Least squares estimation                                                      ####
# Fitted values and cross-validation                                            ####
#   - hat-matrix | -CV statistic | leverage | influential points                ####
# Forecasts and prediction intervals                                            ####
#                                                                               ####
####################################################################################

```

#### Exponential smoothing

```{r Exponential smoothing}

# two Parts : 
#  In the first part (Sections 8.1–8.4) we present the mechanics of the most important exponential smoothing methods, and their application in forecasting time series with various characteristics. This helps us develop an intuition to how these methods work. In this setting, selecting and using a forecasting method may appear to be somewhat ad hoc. The selection of the method is generally based on recognising key components of the time series (trend and seasonal) and the way in which these enter the smoothing method (e.g., in an additive, damped or multiplicative manner).
# 
# In the second part of the chapter (Sections 8.5–8.7) we present the statistical models that underlie exponential smoothing methods. These models generate identical point forecasts to the methods discussed in the first part of the chapter, but also generate prediction intervals. Furthermore, this statistical framework allows for genuine model selection between competing models.

```

###### Part 1
####### Simple exponentail smoothing

```{r}

global_economy 

algeria_economy <- global_economy |>
  filter(Country == "Algeria")
algeria_economy

algeria_economy |> 
  autoplot(Exports) +
  labs(y="% of GDP", title = "Exports from Algeria")

```

The simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES)16. This method is suitable for forecasting data with no clear trend or seasonal pattern.
In above figure, no clear trend and seasonality
So lets go ahead with SES

Note: There is a decline in the last few years, which might suggest a trend. We will consider whether a trended method would be better for this series later in this chapter.)


SES different forms - 
<!-- Weighted average form -->
<!-- Component form (useful to write other major complicated forms) -->
<!-- Flat forecasts -->

Optimisation
  Unlike the regression case (where we have formulas which return the values of the regression coefficients that minimise the SSE), this involves a non-linear minimisation problem, and we need to use an optimisation tool to solve it.

```{r}
# lets fit a model to algerian exports

algeria_economy

# fit
fit <- algeria_economy |>
  model(ETS(Exports ~ error("A") + trend("N") + season("N")))

fit

augment(fit)

# forecast
fc <- fit |>
  forecast(h=5)
fc

# plotting

fc |> 
  autoplot(algeria_economy) +               # no levels provided spo by default 80 & 95%
  geom_line(
    aes(y = .fitted),
    col = '#D55E00',
    data = augment(fit)
  ) +
  labs(
    y = "% of GDP",
    title = "Exports : Algeria"
  ) + 
  guides(colour = "None")

```
The forecasts for the period 2018–2022 are plotted in Figure. Also plotted are one-step-ahead fitted values alongside the data over the period 1960–2017. The large value of  α  in this example is reflected in the large adjustment that takes place in the estimated level  ℓt  at each time. A smaller value of  α  would lead to smaller changes over time, and so the series of fitted values would be smoother.

The prediction intervals shown here are calculated using the methods described in Section 8.7. The prediction intervals show that there is considerable uncertainty in the future exports over the five-year forecast period. So interpreting the point forecasts without accounting for the large uncertainty can be very misleading.

####### Methods with trend
Holt’s linear trend method
Damped trend methods

```{r}
#                                          HLT 
# Australian population

global_economy

aus_economy <- global_economy |>
  filter(Country == "Australia") |>
  mutate(Pop = Population/1e6)           # population to millions

aus_economy

autoplot(aus_economy, Pop) +
  labs(y = "Millions", title = "Australian Population over years")


######################### fitting HLT method

fit <- aus_economy |>
  model(
    AAN = ETS(Pop ~ error("A") + trend("A") + season("N"))        # additive - error + trend and No season
  )
  
fit

# forecast
fc <- fit |> forecast(h=10)
fc

```


```{r}

# Holt's and damped both

aus_economy

aus_economy |>
  model(
    `Holt's method` = ETS(Pop ~ error("A") +                    # holts linear trend - AAN
                       trend("A") + season("N")),
    `Damped Holt's method` = ETS(Pop ~ error("A") +
                       trend("Ad", phi = 0.9) + season("N"))    # damped -> A, Ad, N ||| phi -> Damping parameter
  ) |>
  forecast(h = 15) |>
  autoplot(aus_economy, level = NULL) +
  labs(title = "Australian population",
       y = "Millions") +
  guides(colour = guide_legend(title = "Forecast"))



```
Another Example : Internet usage

```{r}
WWWusage

# Converting to tsibble

ww_usage <- as_tsibble(WWWusage)
ww_usage


ww_usage |>
  autoplot(value) + 
  labs(
    x = "Minute",
    y = "Numb. of Users",
    title = "Internet Usage per Minute"
  )


# We will use time series cross-validation to compare the one-step forecast accuracy of the three methods.

ww_usage

ww_usage |>
  stretch_tsibble(.init = 10) |>                          # increasing training sets (increase sby 1 data                                                                    points and starts from 10 data points in a dataset)
  model(
    SES = ETS(value ~ error("A") + trend("N") + season("N")),         # Eimple exponential (ANN)
    Holt = ETS(value ~ error("A") + trend("A") + season("N")),        # holt linear trend  (AAN)
    Damped = ETS(value ~ error("A") + trend("Ad") +                   # Damped             (A, AD, N)
                   season("N"))
  ) |>
  forecast(h = 1) |>                                                  # 1 step ahead forecasts
  accuracy(ww_usage)                                                  # acccuracy 

```

Damped Holt’s method is best whether you compare MAE or RMSE values or MAPE. So we will proceed with using the damped Holt’s method and apply it to the whole data set to get forecasts for future minutes.

Damped model wins on all key error metrics: RMSE, MAE, MAPE, MASE, RMSSE.
Holt has slightly better ME, MPE, and ACF1 — which might matter if:
You’re highly sensitive to bias (ME/MPE), or
You care a lot about residual independence (ACF1)

* sticking to Damped

```{r}
# Implementing on whole data to have future forecasts

fit <- ww_usage |>
  model(
    Damped = ETS(
      value ~ error("A") + trend("Ad") + season("N"))
  )

fit

# Estimated parameters:
tidy(fit)

# 

```
The smoothing parameter for the slope is estimated to be almost one, indicating that the trend changes to mostly reflect the slope between the last two minutes of internet usage. The value of  α  is very close to one, showing that the level reacts strongly to each new observation.


```{r}
# forecasting

fit |>
  forecast(h = 10) |>
  autoplot(ww_usage) +
  labs(x="Minute", 
       y="Number of users",
       title = "Internet usage per minute")

```
The resulting forecasts look sensible with decreasing trend, which flattens out due to the low value of the damping parameter (0.815), and relatively wide prediction intervals reflecting the variation in the historical data.

Note: 
  In this example, the process of selecting a method was relatively easy as both MSE and MAE comparisons suggested the same method (damped Holt’s). However, sometimes different accuracy measures will suggest different forecasting methods, and then a decision is required as to which forecasting method we prefer to use. As forecasting tasks can vary by many dimensions (length of forecast horizon, size of test set, forecast error measures, frequency of data, etc.), it is unlikely that one method will be better than all others for all forecasting scenarios. What we require from a forecasting method are consistently sensible forecasts, and these should be frequently evaluated against the task at hand.

```{r}


```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```







```{r}
# Plot one time series
aus_retail 
# |>
#   filter(`Series ID`=="A3349640L") |>
#   autoplot(Turnover)



```

```{r}

```

```{r}

# Plot one time series
# aus_retail |>
#   filter(`Series ID`=="A3349640L") |>
#   autoplot(Turnover)

aus_retail <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/aus_retail.csv") |>
  mutate(Month = yearmonth(Month)) |>
  as_tsibble(index = Month, key = c(`Series ID`))


```

```{r}

```
