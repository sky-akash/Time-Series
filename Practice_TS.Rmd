---
title: "Time Series"
author: "Akash Mittal"
date: "2025-08-12"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
# terminal commands to push to github 
# git remote add origin https://github.com/sky-akash/Time-Series.git
# git push -u origin master

# want to know what changed between committs ?
# git log c5e3719..3c5a3e8
# git log --oneline c5e3719..3c5a3e8
# ? what changed from commit A (c5e3719) to commit B (3c5a3e8).

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax
for authoring HTML, PDF, and MS Word documents. For more details on
using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. You can embed an R code chunk like this:

```{r cars}
# ye naam kyun dena ?
#  Naam dene ke 5 faayde:
# 1. Error messages ko samajhne mein aasani hoti hai
# 2. Document ke structure ko samajhna easy ho jaata hai
# 3. Caching ke liye zaroori hota hai
#     {r clean-data, cache=TRUE}
# 4. Output files (like images) ka naam control karna
#     {r scatter-plot}
#     plot(cars)
#     O/p Name will be : scatter-plot-1.png
# 5. Cross-referencing ke liye
#     “See analysis in chunk model-fit”
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
# Code ko output document mein nahi dikhana, lekin output (jaise plot) ko dikhana.
# 
# In simple terms:
# Code chalega ✔️
# 
# Code output aayega ✔️
# 
# Code khud nahi dikhega ❌

#plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to
prevent printing of the R code that generated the plot.

------------------------------------------------------------------------

```{r}
#install.packages("fpp3", dependencies = TRUE)

library(fpp3)         # Loads a collection of packages (including tsibble, fable, feasts) for forecasting and time series analysis.

library(fable)        # Used for forecasting models (like ARIMA, ETS, etc.) with tidy syntax.

library(tibble)       # A modern reimagining of data frames that is more consistent and user-friendly.

library(tsibble)      # Provides a time-aware data structure (tsibble) for tidy time series analysis.

library(feasts)       # Tools for time series feature extraction and decomposition (like STL, ACF, etc.).

library(tsibbledata)  # Provides sample time series datasets (such as `aus_retail`, `global_economy`, etc.).

library(tidyr)        # Helps in tidying data (reshaping, pivoting, separating columns, etc.).

library(ggplot2)      # A popular and flexible system for data visualization using the grammar of graphics.

library(lubridate)    # Makes working with dates and times easier (e.g., parsing, extracting components like year/month).

library(dplyr)        # Used for data manipulation (filtering, selecting, mutating, summarizing, etc.).

library(fabletools)   # Provides tools and infrastructure to support modeling with fable (under the hood utilities).

library(slider)
library(seasonal)
```

## Time Series Graphics

```{r avialable datasets}

data() # lists abvailable datasets (A lot of them in these packages....)

```

```{r aus_retail dataset}

# summary(aus_retail)
str(aus_retail)

```

```{r unique-items}

# aus_retail |> distinct('Series ID')
# 
# aus_retail |> unique('State') # issue as unique wants a vector, and can't evven do aus_retail$'State' as It makes me pass df again to original df

aus_retail |> 
  pull(State) |>
  unique()

# OR 

unique(pull(aus_retail, State))

# OR 

unique(aus_retail$State)      

# OR Use distinct from (deplyr)

aus_retail |> distinct(State) |> pull()             # All aboe ancd this doens give output as tibble but just "A VECTOR"

aus_retail |> distinct(State) # here even output is a tibble

# ###########

aus_retail |> distinct(`Series ID`)
aus_retail |> distinct(State)
aus_retail |> distinct(Industry)

colnames(aus_retail)
head(aus_retail)


aus_retail |> distinct(State, Industry) # distinct also used to in combined series too

```

```{r}

aus_retail |>
  filter(`Series ID` == 'A3349640L') |>
  autoplot(Turnover)

```

```{r}
aus_retail |> 
  filter(`Series ID` == "A3349640L") |>
  model(ETS(Turnover)) |>
  forecast(h = '2 year')

```

```{r}
aus_retail |> 
  filter(`Series ID` == "A3349640L") |>
  model(ETS(Turnover)) |>
  forecast(h = '2 years')
```

```{r ch1}

y <- tsibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110),
  index = Year
)

plot(y)

y

```

```{r}
# converting a dataset to tsibble

# z = data.frame("Month" = "2019 Jan" to "2019 May", # won't work as need to individually write the month year data
#                "observation" = c(50, 23, 45, 34, 67))

z = data.frame(
  "Month" = seq(yearmonth("2019 Jan"), yearmonth("2019 May"), by = 1),
  "observation" = c(50, 23, 45, 34, 67))


z

z |> 
  mutate(Month = yearmonth(Month)) |>
  as_tsibble(index=Month)
# Not yet assigned back to z, so z will still be a df not a tsibble

z

# Notes on ts data
# 1. Creating or cleaning time columns
# If you have a column like "2019 Jan" or "2020 Q2" in character format, you'll use:
# 
# Frequency	Use this function	Example
# Monthly	yearmonth()	yearmonth("2019 Jan")
# Quarterly	yearquarter()	yearquarter("2020 Q2")
# Weekly	yearweek()	yearweek("2021 W05")
# Daily	ymd() or as_date()	ymd("2021-08-12")
# Hourly+	ymd_hms() or as_datetime()	ymd_hms("2021-08-12 13:45:00")

# 2. Converting to a tsibble
# as_tsibble(index = time_column) # time_column must of class mentioned above or POSIXct

# 3. Creating date sequences
# seq(yearmonth("2019 Jan"), yearmonth("2020 Jan"), by = 1)
# seq(yearquarter("2019 Q1"), yearquarter("2020 Q1"), by = 1)
# seq(ymd("2022-01-01"), ymd("2022-12-31"), by = "1 day")

#### Notice the difference between how by is passed for difference base values of time


```

```{r}
# key variables in time series of tsibble 
# allows multiple time series to be stored in a single object
#

olympic_running


# 312*4 [4Y] - 312 rows, 4 columns and period of 4 years
# key -> length, sex [14] -> means 14 differencet time series are there
# The 14 time series in this object are uniquely identified by the keys: the Length and Sex variables.
#

olympic_running |>
  distinct(Length, Sex)

```

```{r deplyr functions}

# mutate(), filter(), select() and summarise() on tsibble object

PBS

key_vars(PBS)

PBS |>
  filter(ATC1=="A")

PBS |>
  filter(ATC1=="A") |>
  select(Month, Concession, Type)

PBS |>
  filter(ATC1=="A") |>
  select(Month, Concession, Type, Cost) |>
  summarize(TotalC=sum(Cost))


PBS |>
  filter(ATC1 == "A") |>
  select(Month, Concession, Type, Cost) |>
  summarize(TotalC = sum(Cost)) |>
  mutate(Cost = TotalC/1e6)

# saving the tsibble

PBS |>
  filter(ATC2 == "A10") |>
  select(Month, Concession, Type, Cost) |>
  summarise(TotalC = sum(Cost)) |>
  mutate(Cost = TotalC / 1e6) -> a10          # saving the tsibble


a10
```

```{r}
has_gaps(PBS)   # Are there any missing time points? # to check for any missing time series data momtnths etc.
# .gaps = FALSE → No months are missing; the time series is complete.
# .gaps = TRUE → Some months are missing in the time series — there's a gap in the timeline.
############

# if any gaps can fill using """""""""fill_gaps()"""""" or """"""na_interpolate()""""""

############
n_keys(PBS)     # How many unique series?
```

```{r csv_to_tsibble}
#

prison <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/prison_population.csv")

summary(prison)
str(prison)

# spc_tbl_ [3,072 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
#  $ Date      : Date[1:3072], format: "2005-03-01" "2005-03-01" "2005-03-01" "2005-03-01" ...
#  $ State     : chr [1:3072] "ACT" "ACT" "ACT" "ACT" ...
#  $ Gender    : chr [1:3072] "Female" "Female" "Female" "Female" ...
#  $ Legal     : chr [1:3072] "Remanded" "Remanded" "Sentenced" "Sentenced" ...
#  $ Indigenous: chr [1:3072] "ATSI" "Non-ATSI" "ATSI" "Non-ATSI" ...
#  $ Count     : num [1:3072] 0 2 0 5 7 58 5 101 51 131 ...
#  - attr(*, "spec")=
#   .. cols(
#   ..   Date = col_date(format = ""),
#   ..   State = col_character(),
#   ..   Gender = col_character(),
#   ..   Legal = col_character(),
#   ..   Indigenous = col_character(),
#   ..   Count = col_double()
#   .. )
#  - attr(*, "problems")=<externalptr> 


prison <- prison |>
  mutate(Quarter = yearquarter(Date)) |>
  select(-Date) |>
  as_tsibble(key = c(State, Gender, Legal, Indigenous),
             index = Quarter)
prison


# More complicated (and unusual) seasonal patterns can be specified using the "period()" function in the "lubridate" package.

```

```{r}
ansett
```

```{r Time plots}

melsyd_economy <- ansett |>
  filter(Airports == "MEL-SYD", Class=="Economy")|>
  mutate(Passengers = Passengers/1000)

melsyd_economy

autoplot(melsyd_economy,Passengers) + # dataset and the column data which has to be plotted
  labs(title = "Ansett airlines economy class",
       subtitle = 'Melbourne-Sydney',
       y = "Passengers ('000) i.e. in Thousands")

```

```{r}

a10

str(a10)
#summary(a10)

autoplot(a10, Cost) +
  labs(y = "Million dollars",
       title = "Australian drug Sales")

a10 |> 
  gg_season(Cost, labels = 'both') +
  labs(y = "Millions Dollars",
       title = "Seasonal Plot : Antidiabetic Drug Sales")

# making seasonal plot, so it automatically plotted the lines as per every year and different lines for each month as data is monthly seasonal


```

```{r}
# data which is seasonal by day

vic_elec

str(vic_elec)
# i.e. half hourly data is given

vic_elec |> 
  gg_season(Demand, period = "day")  +           #  If a string (e.g., "1y" for 1 year, "3m" for 3 months,
                                                # "1d" for 1 day, "1h" for 1 hour, "1min" for 1 minute, "1s"
                                                # for 1 second), it's converted to a Period class object from
                                                # the lubridate package. Note that the data must have at least
                                                # one observation per seasonal period, and the period cannot be
                                                # smaller than the observation interval.
  theme(legend.position = "none") + 
  labs(y = "MWh", title = "Electricity Demand : Victoria")



```

```{r}
# seasonal plots

vic_elec

vic_elec |> gg_season(Demand, period = "week") +
  theme(legend.position = "none") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec |> gg_season(Demand, period = "month") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec |> gg_season(Demand, period = "year") +
  labs(y="MWh", title="Electricity demand: Victoria")

```

```{r}

# # vic_elec |> gg_season(Demand, period = "month")
# # Cannot pass series as strings in gg_subseries , it re
# vic_elec |> gg_subseries(Demand, period = 336) +
#   labs(y = "Million dollars",
#        title = "Australian antidiabetic drug sales")

```

Subseries plots

```{r}
a10

str(a10)
```

```{r}
a10 |>
  gg_season(Cost)

```

```{r subseries plots seasonals}

a10 |>
  gg_subseries(Cost) +
  labs(
    y = "$ (millions)",
    title = "Australian antidiabetic drug sales"
  )

```

```{r tourism australian - sub_series vs sub_seasonal plots}

tourism

str(tourism)

# tbl_ts [24,320 × 5] (S3: tbl_ts/tbl_df/tbl/data.frame)
#  $ Quarter: qtr [1:24320] 1998 Q1, 1998 Q2, 1998 Q3, 1998 Q4, 1999 Q1, 1999 Q2, 1999 Q3, 1999 Q4, 2000 Q1, 2000 Q2, ...
#    ..@ fiscal_start: num 1
#  $ Region : chr [1:24320] "Adelaide" "Adelaide" "Adelaide" "Adelaide" ...
#  $ State  : chr [1:24320] "South Australia" "South Australia" "South Australia" "South Australia" ...
#  $ Purpose: chr [1:24320] "Business" "Business" "Business" "Business" ...
#  $ Trips  : num [1:24320] 135 110 166 127 137 ...

# tourism is tsibble data with Quarter as time frames..so while grouping the hierarchy is State -> Region
# grouping by state we get data for 

holidays <- tourism |>
  filter(Purpose == 'Holiday') |>
  group_by(State) |>
  summarise(Trips=sum(Trips))

holidays

autoplot(holidays, Trips)+
  labs(y = "Trips (thousands") +
  labs(title = "Australian domestic holidays")


gg_season(holidays, Trips) +                      # sub seasonal plot ... and there are eight states (as)
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

holidays |>                                       
  gg_subseries(Trips) +                           # sub series plots ..... time frame is quarterly, and
                                                  # thus 4 quarters for each state
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

```

```{r}
holidays
```

```{r}

vic_elec |>
  filter(year(Time) == 2014) |>
  autoplot(Demand) +
  labs(y = "GW",
       title = "Half-hourly electricity demand: Victoria")

```

```{r}
vic_elec

vic_elec |>
  filter(year(Time) == 2014) |>
  autoplot(Temperature) +
  labs(
    y = "deg C",
    title = "Half Hourly Temperatures (Melbourne)"
  )

# Note: In the dataset, the temperatures are for Melbourne, the largest city in Victoria, while the demand values are for the entire state.

```

```{r}
# scatterplot of series Temp vs demand

vic_elec |>
  filter(year(Time)==2014) |>
  ggplot(aes(x=Temperature, y=Demand)) +
  geom_point() + 
  labs(
    title = "Electricity Demand vs Temp.",
    x = "Temp. deg C Melbourne",
    y = "Electricity  Demand in Victoria"
  )


```

```{r}
# Scatterplot Matrices


visitors <- tourism |>
  group_by(State) |>
  summarise(Trips = sum(Trips))


visitors |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State), scales = "free_y") +
  labs(title = "Australian domestic tourism",
       y= "Overnight trips ('000)")


visitors |>
  pivot_wider(values_from=Trips, names_from=State) |>
  GGally::ggpairs(columns = 2:9)

```

```{r}

unique(visitors$State)

```

```{r lag plots}

aus_production 
str(aus_production)
#summary(aus_production) - error isssue with time series

```

```{r}

recent_production <- aus_production |>
  filter(year(Quarter) >= 1990)

recent_production |>
  feasts::gg_lag(Beer, geom = "path", lags = 1:10, arrow=TRUE) +                   # geom can be "point" or "path" # by fefault there are 9 lags 
  labs(x = "lag(Beer, k)")

```

```{r autocorrelations}

recent_production |>
  feasts::ACF(Beer, lag_max = 50)
  

recent_production |>
  feasts::ACF(Beer, lag_max = 50) |>                        # this will give a tsibble df with lags and acf (And autoplot will
                                        # plot it accordingly)
  autoplot() + 
  labs(title = "Australian Beer Production")
  
  

```

-   peaks in 4 quarters and troughs too.
-   dashed blue line -\> if correlations are significantly differente
    from 0 or not. "Bartlett Bands"

```{r trend and seasonality in ACF}

a10
str(a10)    #  3 monthly seasonal


a10 |>
  feasts::ACF(Cost, lag_max = 50) |>
  autoplot() +
  labs(title="ACT Anti")



```

<!-- exponentially decreasing acf with peaks at every 12 months apart..better check the PACF plots too.  -->

```{r drawing PACF}
a10 |>
  feasts::PACF(Cost, lag_max = 50) |>
  autoplot() +
  labs(title="ACT Anti")

```

```{r White Noise}
set.seed(3)
y <- tsibble(
  sample = 1:50,
  wn = rnorm(50),
  index = sample
)

y 

y |> autoplot(wn) +
  labs(title = "White Noise", y = "")               # y = "" removes label from y axis


y |>
  ACF(wn) |>
  autoplot() + labs(title="White Noise ACF")


```

## Time Series Decomposition

```{r transformation and adjustment}

global_economy

str(global_economy)

tsibbledata::global_economy |>
  filter(Country == "Australia") |>
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y="$US")


global_economy

str(global_economy)

tsibbledata::global_economy |>
  filter(Country == "India") |>
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y="$US")

```

```{r inflation price adjustments}

print_retail <- aus_retail |>
  filter(Industry == "Newspaper and book retailing") |>
  group_by(Industry) |>
  index_by(Year = year(Month)) |>
  summarise(Turnover = sum(Turnover))  # |> mutate(Industry = "Newspaper and book retailing")

print_retail              # newspaper printing turnover data over years
  
aus_economy <- global_economy |>
  filter(Code == "AUS")

aus_economy               # australian economy data oer years including CPI

# both have different years and also we need to mathc data for years so we perform left join to have consistent data

print_retail |>
  left_join(aus_economy, by = "Year") |> 
  mutate(`Inf. Adjusted Turnover` = Turnover/CPI * 100) # |>
  #pivot_longer(c(Turnover, `Inf. Adjusted Turnover`),
   #            values_to = "Turnover")
print_retail

```

```{r}

print_retail |>
  left_join(aus_economy, by = "Year") |> 
  mutate(`Inf. Adjusted Turnover` = Turnover/CPI * 100) |>
  pivot_longer(c(Turnover, `Inf. Adjusted Turnover`), # by long format we create a column with name "name" and values given in this vector, we can remname the coum too, using names_to = `Turnover Type`
              values_to = "Turnover") |>  # turnover is just a name of new column
  mutate(name=factor(name, 
                     levels=c('Turnover', 'Inf. Adjusted Turnover'))) |>
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() + 
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry",
       y = "$AU")
  
  
print_retail


```

Box COX Transformation (allows for negative values which log transform
doesn't) (also helps in converting data to normal form (not always))

```         
  |
  | log(Yt)      if λ = 0  (for log 0, ie. zero remainss zero)
```

Wt = \| \| ((sign(Yt)/Yt)\^(λ) - 1) / λ else \| OR \| (Yt)\^(λ) - 1) /
λ) \# variation neeeds to know difference

A good value of λ is one which makes the size of the seasonal variation
about the same across the whole series, as that makes the forecasting
model simpler.

```{r BoX Cox Transformation & guerrero feature}

# lambda is found using guerrero feature

aus_production |>
  autoplot(Gas)

lambda <- aus_production |>
  fabletools::features(Gas, features = guerrero) |>
  pull(lambda_guerrero)


aus_production |>
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0(
         "Transformed gas production with $\\lambda$ = ",
         round(lambda,2))))



```

```{r Time series components, STL decomposition}

fpp3::us_employment # monthly seasonal with Series_ID as having 148 different series data

us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == 'Retail Trade') |> # title named coumn with valeus being Retail Trade
  select(-Series_ID)

us_retail_employment

autoplot(us_retail_employment,Employed)+
  labs(y = "Persons (thousands)",
       title = "Total Employment in US Retail")


```

<!-- # looks to have trend and some seasonality maybe ? let's divide into parts and check | -->

```{r decomposition}

dcmp <- us_retail_employment |>
  fabletools::model(
                    stl = STL(Employed)
                    )
fabletools::components(dcmp)

str(components(dcmp))

```

<!-- The output above shows the components of an STL decomposition. -->

```{r}

str(components(dcmp))

components(dcmp) |>
  as_tsibble()

components(dcmp) |>
  as_tsibble() |>                                                 # components(dcmp) is a dataframe so need to                                                                         # convert to tsibble
  autoplot(Employed, colour="pink") +
  geom_line(aes(y=trend), colour="darkgreen") +
  labs(
    y = "Persons (k)",
    title = "Total Employment in US Retail (Trend)"
  )

# Seasonally Adjusted Data

# data after removing seasonal compoenet from data

components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail (Seasonally Adjusted)")

```

<!-- Seasonally adjusted plot is wiggly, because it has Trend component and the remainder compoentns (noise) -->

```{r plotting all components}

components(dcmp) |> autoplot()           # here we didn't need it to be tsibble object ?

# lets try by converting it to tsibble

components(dcmp) |>
  as_tsibble() |>
  autoplot()                 # doesn't wor kas required !

# alternative
components(dcmp) |>
  as_tsibble() |>
  pivot_longer(cols = c(trend, season_year, remainder), names_to = "component") |>
  ggplot(aes(x = Month, y = value)) +                   # "|" inside ggplot is used fro faceting/ facets so diff
                                                          # plot  for each component
  geom_line() + 
  facet_wrap(~ component, scales="free_y") + 
  labs(
    y = "Value",
    title = "Decomposed Components"
  ) + 
  theme_minimal()



```

<!-- The grey bars to the right of each panel show the relative scales of the components. Each grey bar represents the same length but because the plots are on different scales, the bars vary in size. -->

```{r}


```

```{r moving averages}

global_economy

global_economy |>
  filter(Country == "Australia")

global_economy |>
  filter(Country == "Australia") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")


################################################

aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE),
    `3-MA` = slider::slide_dbl(Exports, mean,
                .before = 1, .after = 1, .complete = TRUE),
    `7-MA` = slider::slide_dbl(Exports, mean,
                .before = 3, .after = 3, .complete = TRUE)
  ) # 5 period moving average so there will be 2 less observations on top and two less en end

aus_exports

aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "red") +
  geom_line(aes(y = `7-MA`), colour = "green") +
  geom_line(aes(y = `3-MA`), colour = "yellow") +
  labs(y = "% of GDP",
       title = "Total Australian exports")

# slide(.x, .f, ..., .before = 0L, .after = 0L, .step = 1L, .complete = FALSE)
# sliding on a, applying function f.

```

```{r Moving Average of Moving Averages}

aus_production # Quarterly Data

beer <- aus_production |>
  filter(year(Quarter) >= 1992) |>
  select(Quarter, Beer)


beer_ma <-
  beer |>
  mutate(
  
    `4-MA` = slider::slide_dbl(Beer, mean,
                .before = 1, .after = 2, .complete = TRUE),
    
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )

# > >   Y(1)
# > >...Y(1,2) 4MA(1)
#         but 4MA(1) is for this line in real              
# > >   y(1,2) 4MA(2)                                 2*4MA (i.e. average of 2.5 and 2.5 is 3)
#         but 4MA(2) is for this line in real               
# > >   Y(1,2) 
# > >   Y(2)
# > > 

# Lets plot it

beer_ma |>
  autoplot(`Beer`) +
  geom_line(aes(y = `2x4-MA`), colour = "purple")


```

```{r trend-cycle + seasonal }

us_retail_employment # monthly seasonal

us_retail_employment_ma <- us_retail_employment |>
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )


us_retail_employment_ma |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")

```

<!-- Classical decomposition -->

<!-- Additive decomposition \ -->

<!-- Step 1 -->
<!-- If $m$ is an even number, compute the -->
<!-- trend-cycle component $\hat{T}_t$ using a $2 \times m$-MA. If $m$ is an -->
<!-- odd number, compute the trend-cycle component $\hat{T}_t$ using an -->
<!-- $m$-MA. -->

<!-- Step 2 -->
<!-- Calculate the detrended series: $y_t - \hat{T}_t$. -->

<!-- Step 3 -->
<!-- To estimate the seasonal component for each season, simply average the -->
<!-- detrended values for that season. For example, with monthly data, the -->
<!-- seasonal component for March is the average of all the detrended March -->
<!-- values in the data. These seasonal component values are then adjusted to -->
<!-- ensure that they add to zero. The seasonal component is obtained by -->
<!-- stringing together these monthly values, and then replicating the -->
<!-- sequence for each year of data. This gives $\hat{S}_t$. -->

<!-- Step 4 -->
<!-- The remainder component is calculated by subtracting the estimated -->
<!-- seasonal and trend-cycle components: $$ -->
<!-- \hat{R}_t = y_t - \hat{T}_t - \hat{S}_t -->


```{r Classical decomposition}

# feasts pacakege function classical_decomposition
# type - additive or multiplicative
# classical_decomposition(formula, type = c("additive", "multiplicative"), ...)
# ... Other arguments passed to stats::decompose().

us_retail_employment |>
  model(
    feasts::classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

 # earlier we used ... STL Decomposition
# fabletools::model(
#                     stl = STL(Employed)
#                     )
# fabletools::components(dcmp)

```

<!-- Multiplicatie Decomposition -->
<!-- Step 1 -->
<!-- If \( m \) is an even number, compute the trend-cycle component \( \hat{T}_t \) using a \( 2 \times m \)-MA. If \( m \) is an odd number, compute the trend-cycle component \( \hat{T}_t \) using an \( m \)-MA. -->

<!-- Step 2 -->
<!-- Calculate the detrended series:   -->
<!-- \[ -->
<!-- \frac{y_t}{\hat{T}_t} -->
<!-- \] -->

<!-- Step 3 -->
<!-- To estimate the seasonal component for each season, simply average the detrended values for that season. For example, with monthly data, the seasonal index for March is the average of all the detrended March values in the data. These seasonal indexes are then adjusted to ensure that they add to \( m \). The seasonal component is obtained by stringing together these monthly indexes, and then replicating the sequence for each year of data. This gives \( \hat{S}_t \). -->

<!-- Step 4 -->
<!-- The remainder component is calculated by dividing out the estimated seasonal and trend-cycle components:   -->
<!-- \[ -->
<!-- \hat{R}_t = \frac{y_t}{\hat{T}_t \hat{S}_t} -->
<!-- \] -->

```{r Multiplicative decomposition}

us_retail_employment |>
  model(
    feasts::classical_decomposition(Employed, type = "multiplicative")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical Multiplicative decomposition of total
                  US retail employment")

```

<!-- Visually  dont see major different in curves but see the difference in grey bars for Employed and Trend -->

<!-- # X11 and SEATS -->

X-11 -> The X-11 method uses weighted averages over a moving window of the time series. This is used in combination with the RegARIMA model to prepare the data for decomposition. To use the X-11 decomposition method, the x11() function can be used in the model formula

SEATS -> default method for seasonally adjusting the data. This decomposition method can extract seasonality from data with seasonal periods of 2 (biannual), 4 (quarterly), 6 (bimonthly), and 12 (monthly). This method is specified using the seats() function in the model formula.

```{r x11 method of decomposition}

# Deeloped by Official statistics agencies (such as the US Census Bureau and the Australian Bureau of Statistics)

# Most of them use variants of the X-11 method, or the SEATS method, or a combination of the two. These methods are designed specifically to work with quarterly and monthly data, which are the most common series handled by official statistics agencies. They will not handle seasonality of other kinds, such as daily data, or hourly data, or weekly data. 

# There are methods for both ADDITIVE and MULTIPLICATIVE decomposition. 

# x11 decomposition is in X-13ARIMA-SEATS  function of feasts package
# but we also installed seasonal package for this

us_retail_employment

x11_dcmp <- us_retail_employment |>
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) |>
  components()
autoplot(x11_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using X-11.")

#######################################

x11_dcmp

x11_dcmp |>
  ggplot(aes(x = Month)) +
  geom_line(aes(y = Employed, colour = "Data")) +
  geom_line(aes(y = season_adjust,
                colour = "Seasonally Adjusted")) +
  geom_line(aes(y = trend, colour = "Trend")) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail") +
  scale_colour_manual(
    values = c("gray", "#0072B2", "#D55E00"),
    breaks = c("Data", "Seasonally Adjusted", "Trend")
  )

######################################
#         Subseries Plots            #
######################################
x11_dcmp |>
  gg_subseries(trend)

x11_dcmp |>
  gg_subseries(seasonal)

```

```{r SEATS method - Seasonal Extraction in ARIMA Time Series}

seats_dcmp <- us_retail_employment |>
  model(seats = X_13ARIMA_SEATS(Employed ~ seats())) |>
  components()
autoplot(seats_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using SEATS")

# The results are similar though
```

```{r STL Decomposition}

us_retail_employment        # Monthly Series

#################################################################################
#   ?STL     -> Multiple seasonal decomposition by Loess  | from feasts
#   Decompose a time series into seasonal, trend and remainder components. Seasonal components are estimated iteratively using STL. Multiple seasonal periods are allowed. The trend component is computed for the last iteration of STL. Non-seasonal time series are decomposed into trend and remainder only. In this case, supsmu is used to estimate the trend. Optionally, the time series may be Box-Cox transformed before decomposition. Unlike stl, mstl is completely automated.
# 
# STL(formula, iterations = 2, ...)
# trend(window, degree, jump)
# season(period = NULL, window = NULL, degree, jump) | If the window is set to "periodic" or Inf, the seasonal pattern will be fixed. 
#
#################################################################################

us_retail_employment |>
  model(
    feasts::STL(Employed ~ trend(window = 7) +           # span(in lags)of loess window (need be ODD)
                   season(window = "periodic"),         # window is set to "periodic" or Inf, the                                                             seasonal pattern will be fixed.
    robust = TRUE)) |>
  components() |>
  autoplot()

#
#
#  The two main parameters to be chosen when using STL are the trend-cycle window trend(window = ?) and the seasonal window season(window = ?). These control how rapidly the trend-cycle and seasonal components can change. Smaller values allow for more rapid changes. Both trend and seasonal windows should be odd numbers; trend window is the number of consecutive observations to be used when estimating the trend-cycle; season window is the number of consecutive years to be used in estimating each value in the seasonal component. Setting the seasonal window to be infinite is equivalent to forcing the seasonal component to be periodic season(window='periodic') (i.e., identical across years). 
#
#########################################
```


## Time series features

<!-- The feasts package includes functions for Feature Extraction And Statistics from Time Series (hence the name) -->



```{r descriptive features}

# dataset
tourism  # quarterly REgion + State + Purpose + Trips

tourism |>
  features(Trips, list(mean=mean, sd=sd, quantiles=quantile)) |>
  arrange(mean)                                           # will sort in ascending order of mean

#other way

tourism |>
  features(Trips, quantile)


```


```{r ACF features}
# A time series decomposition can be used to measure the strength of trend and seasonality in a time series

# when we have a large collection of time series, and need to find the series with the most trend or the most seasonality

# feat_acf()

tourism |>
  features(Trips, feat_acf)

colnames(tourism |>
  features(Trips, feat_acf)
)

# using these features in plots to identify what type of series are heavily trended and what are most seasonal.
```

```{r}

tourism |>
  features(Trips, feat_stl)

colnames(tourism |>
  features(Trips, feat_stl))


#####

tourism |>
  features(Trips, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))

```
Holiday series are most seasonal.

```{r}
# identifying most seasonal series and plotting it


tourism |>
  features(Trips, feat_stl) |>
  filter(
    seasonal_strength_year == max(seasonal_strength_year)
  ) |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>    # c("State", "Region", "Purpose") is the key :)       ||||||||||    "all", the default, returns every match detected in y. This is the same behavior as SQL.
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() + 
  facet_grid(vars(State, Region, Purpose))

################################################################################
#
# # ?vars
# # vars() is a quoting function that takes inputs to be evaluated in the context of a dataset. These inputs can be:
# 
# variable names
# 
# complex expressions
# 
# In both cases, the results (the vectors that the variable represents or the results of the expressions) are used to form faceting groups.
# 
################################################################################

# # The feat_stl() function returns several more features other than those discussed above.# 
# seasonal_peak_year indicates the timing of the peaks — which month or quarter contains the largest seasonal component. This tells us something about the nature of the seasonality. In the Australian tourism data, if Quarter 3 is the peak seasonal period, then people are travelling to the region in winter, whereas a peak in Quarter 1 suggests that the region is more popular in summer.
# seasonal_trough_year indicates the timing of the troughs — which month or quarter contains the smallest seasonal component.
# spikiness measures the prevalence of spikes in the remainder component  # Rt of the STL decomposition. It is the variance of the leave-one-out variances of Rt.
# linearity measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.
# curvature measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.
# stl_e_acf1 is the first autocorrelation coefficient of the remainder series.
# stl_e_acf10 is the sum of squares of the first ten autocorrelation coefficients of the remainder series.

# ?left_join

```


```{r More Features in feasts}
# 
# The remaining features in the feasts package, not previously discussed, are listed here for reference. The details of some of them are discussed later in the book.
# 
# coef_hurst will calculate the Hurst coefficient of a time series which is a measure of “long memory”. A series with long memory will have significant autocorrelations for many lags.
# feat_spectral will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast. A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0. A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
# box_pierce gives the Box-Pierce statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
# ljung_box gives the Ljung-Box statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
# The  
# k
#  th partial autocorrelation measures the relationship between observations  
# k
#   periods apart after removing the effects of observations between them. So the first partial autocorrelation ( 
# k
# =
# 1
#  ) is identical to the first autocorrelation, because there is nothing between consecutive observations to remove. Partial autocorrelations are discussed in Section 9.5. The feat_pacf function contains several features involving partial autocorrelations including the sum of squares of the first five partial autocorrelations for the original series, the first-differenced series and the second-differenced series. For seasonal data, it also includes the partial autocorrelation at the first seasonal lag.
# unitroot_kpss gives the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) statistic for testing if a series is stationary, and the corresponding p-value. This test is discussed in Section 9.1.
# unitroot_pp gives the Phillips-Perron statistic for testing if a series is non-stationary, and the corresponding p-value.
# unitroot_ndiffs gives the number of differences required to lead to a stationary series based on the KPSS test. This is discussed in Section 9.1
# unitroot_nsdiffs gives the number of seasonal differences required to make a series stationary. This is discussed in Section 9.1.
# var_tiled_mean gives the variances of the “tiled means” (i.e., the means of consecutive non-overlapping blocks of observations). The default tile length is either 10 (for non-seasonal data) or the length of the seasonal period. This is sometimes called the “stability” feature.
# var_tiled_var gives the variances of the “tiled variances” (i.e., the variances of consecutive non-overlapping blocks of observations). This is sometimes called the “lumpiness” feature.
# shift_level_max finds the largest mean shift between two consecutive sliding windows of the time series. This is useful for finding sudden jumps or drops in a time series.
# shift_level_index gives the index at which the largest mean shift occurs.
# shift_var_max finds the largest variance shift between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the volatility of a time series.
# shift_var_index gives the index at which the largest variance shift occurs.
# shift_kl_max finds the largest distributional shift (based on the Kulback-Leibler divergence) between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the distribution of a time series.
# shift_kl_index gives the index at which the largest KL shift occurs.
# n_crossing_points computes the number of times a time series crosses the median.
# longest_flat_spot computes the number of sections of the data where the series is relatively unchanging.
# stat_arch_lm returns the statistic based on the Lagrange Multiplier (LM) test of Engle (1982) for autoregressive conditional heteroscedasticity (ARCH).
# guerrero computes the optimal  
# λ
#   value for a Box-Cox transformation using the Guerrero method (discussed in Section 3.1).


```


```{r Computing all features :)}
################################################################################
################################################################################
###############################                  ###############################
###############################     feasts       ###############################
###############################                  ###############################
################################################################################
################################################################################

tourism_features <- tourism |>
  features(Trips, feature_set(pkgs = "feasts"))

tourism_features

```


```{r}
# all features that involve seasonality, along with the Purpose variable.

library(glue)
x11()

tourism_features |>
  select_at(vars(contains("season"), Purpose)) |>                      # features with season
  mutate(
    seasonal_peak_year = seasonal_peak_year +
      4*(seasonal_peak_year==0),                                  # A conditional check for 0 and then modify it to 4 (without                                                                      using ifelse)
    seasonal_trough_year = seasonal_trough_year +
      4*(seasonal_trough_year==0),
    
    seasonal_peak_year = glue("Q{seasonal_peak_year}"),           # This wraps the numeric value with a "Q" prefix, Q1
    
    seasonal_trough_year = glue("Q{seasonal_trough_year}"),       # wraps numeric value
  ) |>
  GGally::ggpairs(mapping = aes(colour = Purpose))
x11()
```


```{r}

library(broom)

pcs <- tourism_features |>

  select(-State, -Region, -Purpose) |>
  
  prcomp(scale = TRUE) |>                                 # PCA analysis, scaling standardies the features mean=0, stdev=1 
  
  augment(tourism_features)     # augment() from broom adds fitted values/PCA scores back to original data (tourism_features).


pcs |>
  
  ggplot(
    aes(
      x = .fittedPC1,
      y = .fittedPC2,
      col = Purpose)
    ) +
  geom_point() +
  theme(aspect.ratio = 1)

```


```{r}

outliers <- pcs |>
  filter(.fittedPC1 > 10) |>                                # 10 is arbitraray choosen lets say from plot
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers

#> # A tibble: 4 × 5
#>   Region                 State             Purpose  .fittedPC1 .fittedPC2
#>   <chr>                  <chr>             <chr>         <dbl>      <dbl>
#> 1 Australia's North West Western Australia Business       13.4    -11.3  
#> 2 Australia's South West Western Australia Holiday        10.9      0.880
#> 3 Melbourne              Victoria          Holiday        12.3    -10.4  
#> 4 South Coast            New South Wales   Holiday        11.9      9.42
#> 

outliers |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>
  mutate(Series = glue("{State}", "{Region}", "{Purpose}", .sep = "\n\n")) |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = "free") +
  labs(title = "Outlying time series in PC space")

```


```{r}

```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```











```{r}
# Plot one time series
aus_retail 
# |>
#   filter(`Series ID`=="A3349640L") |>
#   autoplot(Turnover)



```

```{r}

```

```{r}

# Plot one time series
# aus_retail |>
#   filter(`Series ID`=="A3349640L") |>
#   autoplot(Turnover)

aus_retail <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/aus_retail.csv") |>
  mutate(Month = yearmonth(Month)) |>
  as_tsibble(index = Month, key = c(`Series ID`))


```

```{r}

```
