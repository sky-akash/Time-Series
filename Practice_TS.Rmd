---
title: "Time Series"
author: "Akash Mittal"
date: "2025-08-12"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
# terminal commands to push to github 
# git remote add origin https://github.com/sky-akash/Time-Series.git
# git push -u origin master

# want to know what changed between committs ?
# git log c5e3719..3c5a3e8
# git log --oneline c5e3719..3c5a3e8
# ? what changed from commit A (c5e3719) to commit B (3c5a3e8).

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax
for authoring HTML, PDF, and MS Word documents. For more details on
using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that
includes both content as well as the output of any embedded R code
chunks within the document. You can embed an R code chunk like this:

```{r cars}
# ye naam kyun dena ?
#  Naam dene ke 5 faayde:
# 1. Error messages ko samajhne mein aasani hoti hai
# 2. Document ke structure ko samajhna easy ho jaata hai
# 3. Caching ke liye zaroori hota hai
#     {r clean-data, cache=TRUE}
# 4. Output files (like images) ka naam control karna
#     {r scatter-plot}
#     plot(cars)
#     O/p Name will be : scatter-plot-1.png
# 5. Cross-referencing ke liye
#     “See analysis in chunk model-fit”
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
# Code ko output document mein nahi dikhana, lekin output (jaise plot) ko dikhana.
# 
# In simple terms:
# Code chalega ✔️
# 
# Code output aayega ✔️
# 
# Code khud nahi dikhega ❌

#plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to
prevent printing of the R code that generated the plot.

------------------------------------------------------------------------

```{r}
#install.packages("fpp3", dependencies = TRUE)

library(fpp3)         # Loads a collection of packages (including tsibble, fable, feasts) for forecasting and time series analysis.

library(fable)        # Used for forecasting models (like ARIMA, ETS, etc.) with tidy syntax.

library(tibble)       # A modern reimagining of data frames that is more consistent and user-friendly.

library(tsibble)      # Provides a time-aware data structure (tsibble) for tidy time series analysis.

library(feasts)       # Tools for time series feature extraction and decomposition (like STL, ACF, etc.).

library(tsibbledata)  # Provides sample time series datasets (such as `aus_retail`, `global_economy`, etc.).

library(tidyr)        # Helps in tidying data (reshaping, pivoting, separating columns, etc.).

library(ggplot2)      # A popular and flexible system for data visualization using the grammar of graphics.

library(lubridate)    # Makes working with dates and times easier (e.g., parsing, extracting components like year/month).

library(dplyr)        # Used for data manipulation (filtering, selecting, mutating, summarizing, etc.).

library(fabletools)   # Provides tools and infrastructure to support modeling with fable (under the hood utilities).

library(slider)
library(seasonal)
```

## Time Series Graphics

```{r avialable datasets}

data() # lists abvailable datasets (A lot of them in these packages....)

```

```{r aus_retail dataset}

# summary(aus_retail)
str(aus_retail)

```

```{r unique-items}

# aus_retail |> distinct('Series ID')
# 
# aus_retail |> unique('State') # issue as unique wants a vector, and can't evven do aus_retail$'State' as It makes me pass df again to original df

aus_retail |> 
  pull(State) |>
  unique()

# OR 

unique(pull(aus_retail, State))

# OR 

unique(aus_retail$State)      

# OR Use distinct from (deplyr)

aus_retail |> distinct(State) |> pull()             # All aboe ancd this doens give output as tibble but just "A VECTOR"

aus_retail |> distinct(State) # here even output is a tibble

# ###########

aus_retail |> distinct(`Series ID`)
aus_retail |> distinct(State)
aus_retail |> distinct(Industry)

colnames(aus_retail)
head(aus_retail)


aus_retail |> distinct(State, Industry) # distinct also used to in combined series too

```

```{r}

aus_retail |>
  filter(`Series ID` == 'A3349640L') |>
  autoplot(Turnover)

```

```{r}
aus_retail |> 
  filter(`Series ID` == "A3349640L") |>
  model(ETS(Turnover)) |>
  forecast(h = '2 year')

```

```{r}
aus_retail |> 
  filter(`Series ID` == "A3349640L") |>
  model(ETS(Turnover)) |>
  forecast(h = '2 years')
```

```{r ch1}

y <- tsibble(
  Year = 2015:2019,
  Observation = c(123, 39, 78, 52, 110),
  index = Year
)

plot(y)

y

```

```{r}
# converting a dataset to tsibble

# z = data.frame("Month" = "2019 Jan" to "2019 May", # won't work as need to individually write the month year data
#                "observation" = c(50, 23, 45, 34, 67))

z = data.frame(
  "Month" = seq(yearmonth("2019 Jan"), yearmonth("2019 May"), by = 1),
  "observation" = c(50, 23, 45, 34, 67))


z

z |> 
  mutate(Month = yearmonth(Month)) |>
  as_tsibble(index=Month)
# Not yet assigned back to z, so z will still be a df not a tsibble

z

# Notes on ts data
# 1. Creating or cleaning time columns
# If you have a column like "2019 Jan" or "2020 Q2" in character format, you'll use:
# 
# Frequency	Use this function	Example
# Monthly	yearmonth()	yearmonth("2019 Jan")
# Quarterly	yearquarter()	yearquarter("2020 Q2")
# Weekly	yearweek()	yearweek("2021 W05")
# Daily	ymd() or as_date()	ymd("2021-08-12")
# Hourly+	ymd_hms() or as_datetime()	ymd_hms("2021-08-12 13:45:00")

# 2. Converting to a tsibble
# as_tsibble(index = time_column) # time_column must of class mentioned above or POSIXct

# 3. Creating date sequences
# seq(yearmonth("2019 Jan"), yearmonth("2020 Jan"), by = 1)
# seq(yearquarter("2019 Q1"), yearquarter("2020 Q1"), by = 1)
# seq(ymd("2022-01-01"), ymd("2022-12-31"), by = "1 day")

#### Notice the difference between how by is passed for difference base values of time


```

```{r}
# key variables in time series of tsibble 
# allows multiple time series to be stored in a single object
#

olympic_running


# 312*4 [4Y] - 312 rows, 4 columns and period of 4 years
# key -> length, sex [14] -> means 14 differencet time series are there
# The 14 time series in this object are uniquely identified by the keys: the Length and Sex variables.
#

olympic_running |>
  distinct(Length, Sex)

```

```{r deplyr functions}

# mutate(), filter(), select() and summarise() on tsibble object

PBS

key_vars(PBS)

PBS |>
  filter(ATC1=="A")

PBS |>
  filter(ATC1=="A") |>
  select(Month, Concession, Type)

PBS |>
  filter(ATC1=="A") |>
  select(Month, Concession, Type, Cost) |>
  summarize(TotalC=sum(Cost))


PBS |>
  filter(ATC1 == "A") |>
  select(Month, Concession, Type, Cost) |>
  summarize(TotalC = sum(Cost)) |>
  mutate(Cost = TotalC/1e6)

# saving the tsibble

PBS |>
  filter(ATC2 == "A10") |>
  select(Month, Concession, Type, Cost) |>
  summarise(TotalC = sum(Cost)) |>
  mutate(Cost = TotalC / 1e6) -> a10          # saving the tsibble


a10
```

```{r}
has_gaps(PBS)   # Are there any missing time points? # to check for any missing time series data momtnths etc.
# .gaps = FALSE → No months are missing; the time series is complete.
# .gaps = TRUE → Some months are missing in the time series — there's a gap in the timeline.
############

# if any gaps can fill using """""""""fill_gaps()"""""" or """"""na_interpolate()""""""

############
n_keys(PBS)     # How many unique series?
```

```{r csv_to_tsibble}
#

prison <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/prison_population.csv")

summary(prison)
str(prison)

# spc_tbl_ [3,072 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
#  $ Date      : Date[1:3072], format: "2005-03-01" "2005-03-01" "2005-03-01" "2005-03-01" ...
#  $ State     : chr [1:3072] "ACT" "ACT" "ACT" "ACT" ...
#  $ Gender    : chr [1:3072] "Female" "Female" "Female" "Female" ...
#  $ Legal     : chr [1:3072] "Remanded" "Remanded" "Sentenced" "Sentenced" ...
#  $ Indigenous: chr [1:3072] "ATSI" "Non-ATSI" "ATSI" "Non-ATSI" ...
#  $ Count     : num [1:3072] 0 2 0 5 7 58 5 101 51 131 ...
#  - attr(*, "spec")=
#   .. cols(
#   ..   Date = col_date(format = ""),
#   ..   State = col_character(),
#   ..   Gender = col_character(),
#   ..   Legal = col_character(),
#   ..   Indigenous = col_character(),
#   ..   Count = col_double()
#   .. )
#  - attr(*, "problems")=<externalptr> 


prison <- prison |>
  mutate(Quarter = yearquarter(Date)) |>
  select(-Date) |>
  as_tsibble(key = c(State, Gender, Legal, Indigenous),
             index = Quarter)
prison


# More complicated (and unusual) seasonal patterns can be specified using the "period()" function in the "lubridate" package.

```

```{r}
ansett
```

```{r Time plots}

melsyd_economy <- ansett |>
  filter(Airports == "MEL-SYD", Class=="Economy")|>
  mutate(Passengers = Passengers/1000)

melsyd_economy

autoplot(melsyd_economy,Passengers) + # dataset and the column data which has to be plotted
  labs(title = "Ansett airlines economy class",
       subtitle = 'Melbourne-Sydney',
       y = "Passengers ('000) i.e. in Thousands")

```

```{r}

a10

str(a10)
#summary(a10)

autoplot(a10, Cost) +
  labs(y = "Million dollars",
       title = "Australian drug Sales")

a10 |> 
  gg_season(Cost, labels = 'both') +
  labs(y = "Millions Dollars",
       title = "Seasonal Plot : Antidiabetic Drug Sales")

# making seasonal plot, so it automatically plotted the lines as per every year and different lines for each month as data is monthly seasonal


```

```{r}
# data which is seasonal by day

vic_elec

str(vic_elec)
# i.e. half hourly data is given

vic_elec |> 
  gg_season(Demand, period = "day")  +           #  If a string (e.g., "1y" for 1 year, "3m" for 3 months,
                                                # "1d" for 1 day, "1h" for 1 hour, "1min" for 1 minute, "1s"
                                                # for 1 second), it's converted to a Period class object from
                                                # the lubridate package. Note that the data must have at least
                                                # one observation per seasonal period, and the period cannot be
                                                # smaller than the observation interval.
  theme(legend.position = "none") + 
  labs(y = "MWh", title = "Electricity Demand : Victoria")



```

```{r}
# seasonal plots

vic_elec

vic_elec |> gg_season(Demand, period = "week") +
  theme(legend.position = "none") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec |> gg_season(Demand, period = "month") +
  labs(y="MWh", title="Electricity demand: Victoria")

vic_elec |> gg_season(Demand, period = "year") +
  labs(y="MWh", title="Electricity demand: Victoria")

```

```{r}

# # vic_elec |> gg_season(Demand, period = "month")
# # Cannot pass series as strings in gg_subseries , it re
# vic_elec |> gg_subseries(Demand, period = 336) +
#   labs(y = "Million dollars",
#        title = "Australian antidiabetic drug sales")

```

Subseries plots

```{r}
a10

str(a10)
```

```{r}
a10 |>
  gg_season(Cost)

```

```{r subseries plots seasonals}

a10 |>
  gg_subseries(Cost) +
  labs(
    y = "$ (millions)",
    title = "Australian antidiabetic drug sales"
  )

```

```{r tourism australian - sub_series vs sub_seasonal plots}

tourism

str(tourism)

# tbl_ts [24,320 × 5] (S3: tbl_ts/tbl_df/tbl/data.frame)
#  $ Quarter: qtr [1:24320] 1998 Q1, 1998 Q2, 1998 Q3, 1998 Q4, 1999 Q1, 1999 Q2, 1999 Q3, 1999 Q4, 2000 Q1, 2000 Q2, ...
#    ..@ fiscal_start: num 1
#  $ Region : chr [1:24320] "Adelaide" "Adelaide" "Adelaide" "Adelaide" ...
#  $ State  : chr [1:24320] "South Australia" "South Australia" "South Australia" "South Australia" ...
#  $ Purpose: chr [1:24320] "Business" "Business" "Business" "Business" ...
#  $ Trips  : num [1:24320] 135 110 166 127 137 ...

# tourism is tsibble data with Quarter as time frames..so while grouping the hierarchy is State -> Region
# grouping by state we get data for 

holidays <- tourism |>
  filter(Purpose == 'Holiday') |>
  group_by(State) |>
  summarise(Trips=sum(Trips))

holidays

autoplot(holidays, Trips)+
  labs(y = "Trips (thousands") +
  labs(title = "Australian domestic holidays")


gg_season(holidays, Trips) +                      # sub seasonal plot ... and there are eight states (as)
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

holidays |>                                       
  gg_subseries(Trips) +                           # sub series plots ..... time frame is quarterly, and
                                                  # thus 4 quarters for each state
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")

```

```{r}
holidays
```

```{r}

vic_elec |>
  filter(year(Time) == 2014) |>
  autoplot(Demand) +
  labs(y = "GW",
       title = "Half-hourly electricity demand: Victoria")

```

```{r}
vic_elec

vic_elec |>
  filter(year(Time) == 2014) |>
  autoplot(Temperature) +
  labs(
    y = "deg C",
    title = "Half Hourly Temperatures (Melbourne)"
  )

# Note: In the dataset, the temperatures are for Melbourne, the largest city in Victoria, while the demand values are for the entire state.

```

```{r}
# scatterplot of series Temp vs demand

vic_elec |>
  filter(year(Time)==2014) |>
  ggplot(aes(x=Temperature, y=Demand)) +
  geom_point() + 
  labs(
    title = "Electricity Demand vs Temp.",
    x = "Temp. deg C Melbourne",
    y = "Electricity  Demand in Victoria"
  )


```

```{r}
# Scatterplot Matrices


visitors <- tourism |>
  group_by(State) |>
  summarise(Trips = sum(Trips))


visitors |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State), scales = "free_y") +
  labs(title = "Australian domestic tourism",
       y= "Overnight trips ('000)")


visitors |>
  pivot_wider(values_from=Trips, names_from=State) |>
  GGally::ggpairs(columns = 2:9)

```

```{r}

unique(visitors$State)

```

```{r lag plots}

aus_production 
str(aus_production)
#summary(aus_production) - error isssue with time series

```

```{r}

recent_production <- aus_production |>
  filter(year(Quarter) >= 1990)

recent_production |>
  feasts::gg_lag(Beer, geom = "path", lags = 1:10, arrow=TRUE) +                   # geom can be "point" or "path" # by fefault there are 9 lags 
  labs(x = "lag(Beer, k)")

```

```{r autocorrelations}

recent_production |>
  feasts::ACF(Beer, lag_max = 50)
  

recent_production |>
  feasts::ACF(Beer, lag_max = 50) |>                        # this will give a tsibble df with lags and acf (And autoplot will
                                        # plot it accordingly)
  autoplot() + 
  labs(title = "Australian Beer Production")
  
  

```

-   peaks in 4 quarters and troughs too.
-   dashed blue line -\> if correlations are significantly differente
    from 0 or not. "Bartlett Bands"

```{r trend and seasonality in ACF}

a10
str(a10)    #  3 monthly seasonal


a10 |>
  feasts::ACF(Cost, lag_max = 50) |>
  autoplot() +
  labs(title="ACT Anti")



```

<!-- exponentially decreasing acf with peaks at every 12 months apart..better check the PACF plots too.  -->

```{r drawing PACF}
a10 |>
  feasts::PACF(Cost, lag_max = 50) |>
  autoplot() +
  labs(title="ACT Anti")

```

```{r White Noise}
set.seed(3)
y <- tsibble(
  sample = 1:50,
  wn = rnorm(50),
  index = sample
)

y 

y |> autoplot(wn) +
  labs(title = "White Noise", y = "")               # y = "" removes label from y axis


y |>
  ACF(wn) |>
  autoplot() + labs(title="White Noise ACF")


```

## Time Series Decomposition

```{r transformation and adjustment}

global_economy

str(global_economy)

tsibbledata::global_economy |>
  filter(Country == "Australia") |>
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y="$US")


global_economy

str(global_economy)

tsibbledata::global_economy |>
  filter(Country == "India") |>
  autoplot(GDP/Population) +
  labs(title = "GDP per capita", y="$US")

```

```{r inflation price adjustments}

print_retail <- aus_retail |>
  filter(Industry == "Newspaper and book retailing") |>
  group_by(Industry) |>
  index_by(Year = year(Month)) |>
  summarise(Turnover = sum(Turnover))  # |> mutate(Industry = "Newspaper and book retailing")

print_retail              # newspaper printing turnover data over years
  
aus_economy <- global_economy |>
  filter(Code == "AUS")

aus_economy               # australian economy data oer years including CPI

# both have different years and also we need to mathc data for years so we perform left join to have consistent data

print_retail |>
  left_join(aus_economy, by = "Year") |> 
  mutate(`Inf. Adjusted Turnover` = Turnover/CPI * 100) # |>
  #pivot_longer(c(Turnover, `Inf. Adjusted Turnover`),
   #            values_to = "Turnover")
print_retail

```

```{r}

print_retail |>
  left_join(aus_economy, by = "Year") |> 
  mutate(`Inf. Adjusted Turnover` = Turnover/CPI * 100) |>
  pivot_longer(c(Turnover, `Inf. Adjusted Turnover`), # by long format we create a column with name "name" and values given in this vector, we can remname the coum too, using names_to = `Turnover Type`
              values_to = "Turnover") |>  # turnover is just a name of new column
  mutate(name=factor(name, 
                     levels=c('Turnover', 'Inf. Adjusted Turnover'))) |>
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() + 
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry",
       y = "$AU")
  
  
print_retail


```

Box COX Transformation (allows for negative values which log transform
doesn't) (also helps in converting data to normal form (not always))

```         
  |
  | log(Yt)      if λ = 0  (for log 0, ie. zero remainss zero)
```

Wt = \| \| ((sign(Yt)/Yt)\^(λ) - 1) / λ else \| OR \| (Yt)\^(λ) - 1) /
λ) \# variation neeeds to know difference

A good value of λ is one which makes the size of the seasonal variation
about the same across the whole series, as that makes the forecasting
model simpler.

```{r BoX Cox Transformation & guerrero feature}

# lambda is found using guerrero feature

aus_production |>
  autoplot(Gas)

lambda <- aus_production |>
  fabletools::features(Gas, features = guerrero) |>
  pull(lambda_guerrero)


aus_production |>
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0(
         "Transformed gas production with $\\lambda$ = ",
         round(lambda,2))))



```

```{r Time series components, STL decomposition}

fpp3::us_employment # monthly seasonal with Series_ID as having 148 different series data

us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == 'Retail Trade') |> # title named coumn with valeus being Retail Trade
  select(-Series_ID)

us_retail_employment

autoplot(us_retail_employment,Employed)+
  labs(y = "Persons (thousands)",
       title = "Total Employment in US Retail")


```

<!-- # looks to have trend and some seasonality maybe ? let's divide into parts and check | -->

```{r decomposition}

dcmp <- us_retail_employment |>
  fabletools::model(
                    stl = STL(Employed)
                    )
fabletools::components(dcmp)

str(components(dcmp))

```

<!-- The output above shows the components of an STL decomposition. -->

```{r}

str(components(dcmp))

components(dcmp) |>
  as_tsibble()

components(dcmp) |>
  as_tsibble() |>                                                 # components(dcmp) is a dataframe so need to                                                                         # convert to tsibble
  autoplot(Employed, colour="pink") +
  geom_line(aes(y=trend), colour="darkgreen") +
  labs(
    y = "Persons (k)",
    title = "Total Employment in US Retail (Trend)"
  )

# Seasonally Adjusted Data

# data after removing seasonal compoenet from data

components(dcmp) |>
  as_tsibble() |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y=season_adjust), colour = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail (Seasonally Adjusted)")

```

<!-- Seasonally adjusted plot is wiggly, because it has Trend component and the remainder compoentns (noise) -->

```{r plotting all components}

components(dcmp) |> autoplot()           # here we didn't need it to be tsibble object ?

# lets try by converting it to tsibble

components(dcmp) |>
  as_tsibble() |>
  autoplot()                 # doesn't wor kas required !

# alternative
components(dcmp) |>
  as_tsibble() |>
  pivot_longer(cols = c(trend, season_year, remainder), names_to = "component") |>
  ggplot(aes(x = Month, y = value)) +                   # "|" inside ggplot is used fro faceting/ facets so diff
                                                          # plot  for each component
  geom_line() + 
  facet_wrap(~ component, scales="free_y") + 
  labs(
    y = "Value",
    title = "Decomposed Components"
  ) + 
  theme_minimal()



```

<!-- The grey bars to the right of each panel show the relative scales of the components. Each grey bar represents the same length but because the plots are on different scales, the bars vary in size. -->

```{r}


```

```{r moving averages}

global_economy

global_economy |>
  filter(Country == "Australia")

global_economy |>
  filter(Country == "Australia") |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Total Australian exports")


################################################

aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(Exports, mean,
                .before = 2, .after = 2, .complete = TRUE),
    `3-MA` = slider::slide_dbl(Exports, mean,
                .before = 1, .after = 1, .complete = TRUE),
    `7-MA` = slider::slide_dbl(Exports, mean,
                .before = 3, .after = 3, .complete = TRUE)
  ) # 5 period moving average so there will be 2 less observations on top and two less en end

aus_exports

aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "red") +
  geom_line(aes(y = `7-MA`), colour = "green") +
  geom_line(aes(y = `3-MA`), colour = "yellow") +
  labs(y = "% of GDP",
       title = "Total Australian exports")

# slide(.x, .f, ..., .before = 0L, .after = 0L, .step = 1L, .complete = FALSE)
# sliding on a, applying function f.

```

```{r Moving Average of Moving Averages}

aus_production # Quarterly Data

beer <- aus_production |>
  filter(year(Quarter) >= 1992) |>
  select(Quarter, Beer)


beer_ma <-
  beer |>
  mutate(
  
    `4-MA` = slider::slide_dbl(Beer, mean,
                .before = 1, .after = 2, .complete = TRUE),
    
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )

# > >   Y(1)
# > >...Y(1,2) 4MA(1)
#         but 4MA(1) is for this line in real              
# > >   y(1,2) 4MA(2)                                 2*4MA (i.e. average of 2.5 and 2.5 is 3)
#         but 4MA(2) is for this line in real               
# > >   Y(1,2) 
# > >   Y(2)
# > > 

# Lets plot it

beer_ma |>
  autoplot(`Beer`) +
  geom_line(aes(y = `2x4-MA`), colour = "purple")


```

```{r trend-cycle + seasonal }

us_retail_employment # monthly seasonal

us_retail_employment_ma <- us_retail_employment |>
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )


us_retail_employment_ma |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")

```

<!-- Classical decomposition -->

<!-- Additive decomposition \ -->

<!-- Step 1 -->
<!-- If $m$ is an even number, compute the -->
<!-- trend-cycle component $\hat{T}_t$ using a $2 \times m$-MA. If $m$ is an -->
<!-- odd number, compute the trend-cycle component $\hat{T}_t$ using an -->
<!-- $m$-MA. -->

<!-- Step 2 -->
<!-- Calculate the detrended series: $y_t - \hat{T}_t$. -->

<!-- Step 3 -->
<!-- To estimate the seasonal component for each season, simply average the -->
<!-- detrended values for that season. For example, with monthly data, the -->
<!-- seasonal component for March is the average of all the detrended March -->
<!-- values in the data. These seasonal component values are then adjusted to -->
<!-- ensure that they add to zero. The seasonal component is obtained by -->
<!-- stringing together these monthly values, and then replicating the -->
<!-- sequence for each year of data. This gives $\hat{S}_t$. -->

<!-- Step 4 -->
<!-- The remainder component is calculated by subtracting the estimated -->
<!-- seasonal and trend-cycle components: $$ -->
<!-- \hat{R}_t = y_t - \hat{T}_t - \hat{S}_t -->


```{r Classical decomposition}

# feasts pacakege function classical_decomposition
# type - additive or multiplicative
# classical_decomposition(formula, type = c("additive", "multiplicative"), ...)
# ... Other arguments passed to stats::decompose().

us_retail_employment |>
  model(
    feasts::classical_decomposition(Employed, type = "additive")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")

 # earlier we used ... STL Decomposition
# fabletools::model(
#                     stl = STL(Employed)
#                     )
# fabletools::components(dcmp)

```

<!-- Multiplicatie Decomposition -->
<!-- Step 1 -->
<!-- If \( m \) is an even number, compute the trend-cycle component \( \hat{T}_t \) using a \( 2 \times m \)-MA. If \( m \) is an odd number, compute the trend-cycle component \( \hat{T}_t \) using an \( m \)-MA. -->

<!-- Step 2 -->
<!-- Calculate the detrended series:   -->
<!-- \[ -->
<!-- \frac{y_t}{\hat{T}_t} -->
<!-- \] -->

<!-- Step 3 -->
<!-- To estimate the seasonal component for each season, simply average the detrended values for that season. For example, with monthly data, the seasonal index for March is the average of all the detrended March values in the data. These seasonal indexes are then adjusted to ensure that they add to \( m \). The seasonal component is obtained by stringing together these monthly indexes, and then replicating the sequence for each year of data. This gives \( \hat{S}_t \). -->

<!-- Step 4 -->
<!-- The remainder component is calculated by dividing out the estimated seasonal and trend-cycle components:   -->
<!-- \[ -->
<!-- \hat{R}_t = \frac{y_t}{\hat{T}_t \hat{S}_t} -->
<!-- \] -->

```{r Multiplicative decomposition}

us_retail_employment |>
  model(
    feasts::classical_decomposition(Employed, type = "multiplicative")
  ) |>
  components() |>
  autoplot() +
  labs(title = "Classical Multiplicative decomposition of total
                  US retail employment")

```

<!-- Visually  dont see major different in curves but see the difference in grey bars for Employed and Trend -->

<!-- # X11 and SEATS -->

X-11 -> The X-11 method uses weighted averages over a moving window of the time series. This is used in combination with the RegARIMA model to prepare the data for decomposition. To use the X-11 decomposition method, the x11() function can be used in the model formula

SEATS -> default method for seasonally adjusting the data. This decomposition method can extract seasonality from data with seasonal periods of 2 (biannual), 4 (quarterly), 6 (bimonthly), and 12 (monthly). This method is specified using the seats() function in the model formula.

```{r x11 method of decomposition}

# Deeloped by Official statistics agencies (such as the US Census Bureau and the Australian Bureau of Statistics)

# Most of them use variants of the X-11 method, or the SEATS method, or a combination of the two. These methods are designed specifically to work with quarterly and monthly data, which are the most common series handled by official statistics agencies. They will not handle seasonality of other kinds, such as daily data, or hourly data, or weekly data. 

# There are methods for both ADDITIVE and MULTIPLICATIVE decomposition. 

# x11 decomposition is in X-13ARIMA-SEATS  function of feasts package
# but we also installed seasonal package for this

us_retail_employment

x11_dcmp <- us_retail_employment |>
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) |>
  components()
autoplot(x11_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using X-11.")

#######################################

x11_dcmp

x11_dcmp |>
  ggplot(aes(x = Month)) +
  geom_line(aes(y = Employed, colour = "Data")) +
  geom_line(aes(y = season_adjust,
                colour = "Seasonally Adjusted")) +
  geom_line(aes(y = trend, colour = "Trend")) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail") +
  scale_colour_manual(
    values = c("gray", "#0072B2", "#D55E00"),
    breaks = c("Data", "Seasonally Adjusted", "Trend")
  )

######################################
#         Subseries Plots            #
######################################
x11_dcmp |>
  gg_subseries(trend)

x11_dcmp |>
  gg_subseries(seasonal)

```

```{r SEATS method - Seasonal Extraction in ARIMA Time Series}

seats_dcmp <- us_retail_employment |>
  model(seats = X_13ARIMA_SEATS(Employed ~ seats())) |>
  components()
autoplot(seats_dcmp) +
  labs(title =
    "Decomposition of total US retail employment using SEATS")

# The results are similar though
```

```{r STL Decomposition}

us_retail_employment        # Monthly Series

#################################################################################
#   ?STL     -> Multiple seasonal decomposition by Loess  | from feasts
#   Decompose a time series into seasonal, trend and remainder components. Seasonal components are estimated iteratively using STL. Multiple seasonal periods are allowed. The trend component is computed for the last iteration of STL. Non-seasonal time series are decomposed into trend and remainder only. In this case, supsmu is used to estimate the trend. Optionally, the time series may be Box-Cox transformed before decomposition. Unlike stl, mstl is completely automated.
# 
# STL(formula, iterations = 2, ...)
# trend(window, degree, jump)
# season(period = NULL, window = NULL, degree, jump) | If the window is set to "periodic" or Inf, the seasonal pattern will be fixed. 
#
#################################################################################

us_retail_employment |>
  model(
    feasts::STL(Employed ~ trend(window = 7) +           # span(in lags)of loess window (need be ODD)
                   season(window = "periodic"),         # window is set to "periodic" or Inf, the                                                             seasonal pattern will be fixed.
    robust = TRUE)) |>
  components() |>
  autoplot()

#
#
#  The two main parameters to be chosen when using STL are the trend-cycle window trend(window = ?) and the seasonal window season(window = ?). These control how rapidly the trend-cycle and seasonal components can change. Smaller values allow for more rapid changes. Both trend and seasonal windows should be odd numbers; trend window is the number of consecutive observations to be used when estimating the trend-cycle; season window is the number of consecutive years to be used in estimating each value in the seasonal component. Setting the seasonal window to be infinite is equivalent to forcing the seasonal component to be periodic season(window='periodic') (i.e., identical across years). 
#
#########################################
```


## Time series features

<!-- The feasts package includes functions for Feature Extraction And Statistics from Time Series (hence the name) -->



```{r descriptive features}

# dataset
tourism  # quarterly REgion + State + Purpose + Trips

tourism |>
  features(Trips, list(mean=mean, sd=sd, quantiles=quantile)) |>
  arrange(mean)                                           # will sort in ascending order of mean

#other way

tourism |>
  features(Trips, quantile)


```


```{r ACF features}
# A time series decomposition can be used to measure the strength of trend and seasonality in a time series

# when we have a large collection of time series, and need to find the series with the most trend or the most seasonality

# feat_acf()

tourism |>
  features(Trips, feat_acf)

colnames(tourism |>
  features(Trips, feat_acf)
)

# using these features in plots to identify what type of series are heavily trended and what are most seasonal.
```

```{r}

tourism |>
  features(Trips, feat_stl)

colnames(tourism |>
  features(Trips, feat_stl))


#####

tourism |>
  features(Trips, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))

```
Holiday series are most seasonal.

```{r}
# identifying most seasonal series and plotting it


tourism |>
  features(Trips, feat_stl) |>
  filter(
    seasonal_strength_year == max(seasonal_strength_year)
  ) |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>    # c("State", "Region", "Purpose") is the key :)       ||||||||||    "all", the default, returns every match detected in y. This is the same behavior as SQL.
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() + 
  facet_grid(vars(State, Region, Purpose))

################################################################################
#
# # ?vars
# # vars() is a quoting function that takes inputs to be evaluated in the context of a dataset. These inputs can be:
# 
# variable names
# 
# complex expressions
# 
# In both cases, the results (the vectors that the variable represents or the results of the expressions) are used to form faceting groups.
# 
################################################################################

# # The feat_stl() function returns several more features other than those discussed above.# 
# seasonal_peak_year indicates the timing of the peaks — which month or quarter contains the largest seasonal component. This tells us something about the nature of the seasonality. In the Australian tourism data, if Quarter 3 is the peak seasonal period, then people are travelling to the region in winter, whereas a peak in Quarter 1 suggests that the region is more popular in summer.
# seasonal_trough_year indicates the timing of the troughs — which month or quarter contains the smallest seasonal component.
# spikiness measures the prevalence of spikes in the remainder component  # Rt of the STL decomposition. It is the variance of the leave-one-out variances of Rt.
# linearity measures the linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component.
# curvature measures the curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.
# stl_e_acf1 is the first autocorrelation coefficient of the remainder series.
# stl_e_acf10 is the sum of squares of the first ten autocorrelation coefficients of the remainder series.

# ?left_join

```


```{r More Features in feasts}
# 
# The remaining features in the feasts package, not previously discussed, are listed here for reference. The details of some of them are discussed later in the book.
# 
# coef_hurst will calculate the Hurst coefficient of a time series which is a measure of “long memory”. A series with long memory will have significant autocorrelations for many lags.
# feat_spectral will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast. A series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0. A series that is very noisy (and so is difficult to forecast) will have entropy close to 1.
# box_pierce gives the Box-Pierce statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
# ljung_box gives the Ljung-Box statistic for testing if a time series is white noise, and the corresponding p-value. This test is discussed in Section 5.4.
# The  
# k
#  th partial autocorrelation measures the relationship between observations  
# k
#   periods apart after removing the effects of observations between them. So the first partial autocorrelation ( 
# k
# =
# 1
#  ) is identical to the first autocorrelation, because there is nothing between consecutive observations to remove. Partial autocorrelations are discussed in Section 9.5. The feat_pacf function contains several features involving partial autocorrelations including the sum of squares of the first five partial autocorrelations for the original series, the first-differenced series and the second-differenced series. For seasonal data, it also includes the partial autocorrelation at the first seasonal lag.
# unitroot_kpss gives the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) statistic for testing if a series is stationary, and the corresponding p-value. This test is discussed in Section 9.1.
# unitroot_pp gives the Phillips-Perron statistic for testing if a series is non-stationary, and the corresponding p-value.
# unitroot_ndiffs gives the number of differences required to lead to a stationary series based on the KPSS test. This is discussed in Section 9.1
# unitroot_nsdiffs gives the number of seasonal differences required to make a series stationary. This is discussed in Section 9.1.
# var_tiled_mean gives the variances of the “tiled means” (i.e., the means of consecutive non-overlapping blocks of observations). The default tile length is either 10 (for non-seasonal data) or the length of the seasonal period. This is sometimes called the “stability” feature.
# var_tiled_var gives the variances of the “tiled variances” (i.e., the variances of consecutive non-overlapping blocks of observations). This is sometimes called the “lumpiness” feature.
# shift_level_max finds the largest mean shift between two consecutive sliding windows of the time series. This is useful for finding sudden jumps or drops in a time series.
# shift_level_index gives the index at which the largest mean shift occurs.
# shift_var_max finds the largest variance shift between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the volatility of a time series.
# shift_var_index gives the index at which the largest variance shift occurs.
# shift_kl_max finds the largest distributional shift (based on the Kulback-Leibler divergence) between two consecutive sliding windows of the time series. This is useful for finding sudden changes in the distribution of a time series.
# shift_kl_index gives the index at which the largest KL shift occurs.
# n_crossing_points computes the number of times a time series crosses the median.
# longest_flat_spot computes the number of sections of the data where the series is relatively unchanging.
# stat_arch_lm returns the statistic based on the Lagrange Multiplier (LM) test of Engle (1982) for autoregressive conditional heteroscedasticity (ARCH).
# guerrero computes the optimal  
# λ
#   value for a Box-Cox transformation using the Guerrero method (discussed in Section 3.1).


```


```{r Computing all features :)}
################################################################################
################################################################################
###############################                  ###############################
###############################     feasts       ###############################
###############################                  ###############################
################################################################################
################################################################################

tourism_features <- tourism |>
  features(Trips, feature_set(pkgs = "feasts"))

tourism_features

```


```{r}
# all features that involve seasonality, along with the Purpose variable.

library(glue)
x11()

tourism_features |>
  select_at(vars(contains("season"), Purpose)) |>                      # features with season
  mutate(
    seasonal_peak_year = seasonal_peak_year +
      4*(seasonal_peak_year==0),                                  # A conditional check for 0 and then modify it to 4 (without                                                                      using ifelse)
    seasonal_trough_year = seasonal_trough_year +
      4*(seasonal_trough_year==0),
    
    seasonal_peak_year = glue("Q{seasonal_peak_year}"),           # This wraps the numeric value with a "Q" prefix, Q1
    
    seasonal_trough_year = glue("Q{seasonal_trough_year}"),       # wraps numeric value
  ) |>
  GGally::ggpairs(mapping = aes(colour = Purpose))
x11()
```


```{r}

library(broom)

pcs <- tourism_features |>

  select(-State, -Region, -Purpose) |>
  
  prcomp(scale = TRUE) |>                                 # PCA analysis, scaling standardies the features mean=0, stdev=1 
  
  augment(tourism_features)     # augment() from broom adds fitted values/PCA scores back to original data (tourism_features).


pcs |>
  
  ggplot(
    aes(
      x = .fittedPC1,
      y = .fittedPC2,
      col = Purpose)
    ) +
  geom_point() +
  theme(aspect.ratio = 1)

```


```{r}

outliers <- pcs |>
  filter(.fittedPC1 > 10) |>                                # 10 is arbitraray choosen lets say from plot
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers

#> # A tibble: 4 × 5
#>   Region                 State             Purpose  .fittedPC1 .fittedPC2
#>   <chr>                  <chr>             <chr>         <dbl>      <dbl>
#> 1 Australia's North West Western Australia Business       13.4    -11.3  
#> 2 Australia's South West Western Australia Holiday        10.9      0.880
#> 3 Melbourne              Victoria          Holiday        12.3    -10.4  
#> 4 South Coast            New South Wales   Holiday        11.9      9.42
#> 

outliers |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>
  mutate(Series = glue("{State}", "{Region}", "{Purpose}", .sep = "\n\n")) |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = "free") +
  labs(title = "Outlying time series in PC space")

```
### Forecaster's Toolbox

To illustrate the process, we will fit linear trend models to national GDP data stored in global_economy.

```{r tidy forecasting window}

# Data preparation (tidy)

tsibbledata::global_economy

gdppc <- tsibbledata::global_economy |>
  dplyr::mutate(GDP_per_capita = GDP/Population)

gdppc

# Plotting for visaulization (1 country here) sweden

gdppc |>
  filter(Country == "Sweden") |>
  autoplot(GDP_per_capita) +
  labs(y="$US", title = "GDP per capita of Sweden")
  
###################################################################################
# Model specification

# a lot of available models to choose from 
# Models in fable are specified using model functions, which each use a formula (y ~ x) interface. 

########################## TSLM(GDP_per_capita ~ trend()) #########################

# In this case the model function is TSLM() (time series linear model), the response variable is GDP_per_capita and it is being modelled using trend() (a “special” function specifying a linear trend when it is used within TSLM()). We will be taking a closer look at how each model can be specified in their respective sections.
# 
###################################################################################

# Training model estiamting :)

fit <- gdppc |>
  model(trend_model = TSLM(GDP_per_capita ~ trend()))

fit


# Model performance (evaluation)

# for that we will use diagnostics (later)

# Forecasts
#
######### produce the forecasts using forecast()

fit |> forecast(h = "3 years")


# plotting forecasts

fit |>
  forecast(h = "3 years") |>
  filter(Country == "Sweden") |>
  autoplot(gdppc) +
  labs(y = "$US", title = "GDP per capita for Sweden")


```


```{r forecasting methods}

# just model syntaxes here

# 4 simple benchmarking methods :)
# 1. Mean Method
# 2. naive Forecast
# 3. Seasonal Naive
# 4. Drift 

aus_production

# going for bricks production data

bricks <- aus_production|>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Bricks)
bricks

# going for beer data too

beer <- aus_production |>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Beer)
beer

# 1. Mean Method
bricks |> fabletools::model(MEAN(Bricks))
beer |> fabletools::model(MEAN(Beer))


# 2. naive Forecast
bricks |> model(NAIVE(Bricks))
beer |> model(NAIVE(Beer))

bricks |> model(RW(Bricks))

# a naïve forecast is optimal when data follow a random walk (Section 9.1), 
# these are also called random walk forecasts and the RW() function can be used instead of NAIVE

# 3. Seasonal Naive
bricks |> model(SNAIVE(Bricks ~ lag("year")))
# The lag() function is optional here as bricks is quarterly data and so a seasonal naïve method will need a one-year lag.


# 4. Drift 
bricks |> model(RW(Bricks ~ drift()))


```


```{r}
# fitting proper models with forecasts (for Beer)

# Set training data from 1970 to 2006
train <- aus_production |>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Beer)

# Fit the Model
beer_fit <- train |>
  model(
    Mean = MEAN(Beer),
    NAIVE = NAIVE(Beer),
    SNAIVE = SNAIVE(Beer),            # The lag special is used to specify the lag order for the                                          random walk process. If left out, this special will                                               automatically be included.
    DRIFT = RW(Beer ~ drift())
    
  )

# generating forecasts for 14 quarters

beer_fc <- beer_fit |> 
  forecast(h=30)

# Plotting forecasts
beer_fc |> 
  autoplot(train, level=NULL) +              # level is for confidence interval, can accpt vector (def. 85 - 90%)
  autolayer(
    filter_index(aus_production, "2004 Q1" ~ .),
    colour = "Black"
  ) + 
  labs(
    y = "Mega L",
    title = "Forecasts for Quarterly Beer Production"
  ) +
  guides(
    colour = guide_legend(title="Forecast")
  )



```


```{r Google’s daily closing stock price}

# Re-index based on trading days

# ?gafa_stock # Historical stock prices from 2014-2018 for Google, Amazon, Facebook and Apple ($USD).

google_stock <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) >= 2015)|>
  mutate(day = dplyr::row_number()) |>
  update_tsibble(index = day, regular = TRUE)


# Filter the year of interest
google_2015 <- google_stock |> filter(year(Date) == 2015)


# Fit the models
google_fit <- google_2015 |>
  model(
    Mean = MEAN(Close),
    Naive = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )


# Produce forecasts for the trading days in January 2016

google_jan_2016 <- google_stock |>
  filter(yearmonth(Date) == yearmonth("2016 Jan"))       # for january forecasts only

google_fc <- google_fit |>
  forecast(new_data = google_jan_2016)

################################################################################
# Note :::::::::::::::
# 
# Forecast using new_data: 
# This is used when you want to forecast specific future time points, especially if:
# Your time series is irregular (like stock market data — only trading days).
# 
# You want forecasts for custom dates (not just the next h steps).
############
#             forecast(model, new_data = some_data)
############
# "some_data" must be a "data frame" or "tsibble" that includes the future time points you want forecasts for.
# 
# You manually provide the time index (like dates) for which you want forecasts.
# se when time is irregular or you want to forecast specific dates.
#
################################################################################


# Plot the forecasts
google_fc |>
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, colour = "black") +
  labs(y = "$US",
       title = "Google daily closing stock prices",
       subtitle = "(Jan 2015 - Jan 2016)") +
  guides(colour = guide_legend(title = "Forecast"))


```


```{r fitted values and residuals}

# Augment - apply augment() to this object to compute the fitted values and residuals for all models.

augment(beer_fit)

################################################################################
#
# There are three new columns added to the original data:
#
# .fitted contains the fitted values;
# .resid contains the residuals;
# .innov contains the “innovation residuals” which, in this case, are identical to the regular residuals.
# 
################################################################################

# #################################################################################
# # Note: 
# 
# Residuals are useful in checking whether a model has adequately captured the information in the data. For this purpose, we use innovation residuals.
# 
# If patterns are observable in the innovation residuals, the model can probably be improved. We will look at some tools for exploring patterns in residuals in the next section.
# 
# #################################################################################

```

##### Residual diagnostics

```{r}
# good forecasting
### uncorrelated inoovation residuals
### zero mean of innov. residuals

# Further good/ useful to have ->
# constant variance on innov resid 
# innov. resid. normally distributed
# 

# Forecasting Google daily closing stock prices
autoplot(google_2015, Close) +
  labs(y = "$US",
       title = "Google daily closing stock prices in 2015")

# The following graph shows the Google daily closing stock price for trading days during 2015. The large jump corresponds to 17 July 2015 when the price jumped 16% due to unexpectedly strong second quarter results



aug <- google_2015 |>
  fabletools::model(NAIVE(Close)) |>
  augment()                           # to get residual data
autoplot(aug, .innov) +
  labs(y = "$US",
       title = "Residuals from the naïve method")


# hostogram

aug |>
  ggplot(aes(x = .innov)) +
  geom_histogram() +
  labs(title = "Histogram of residuals")

# ACF
aug |>
  feasts::ACF(.innov) |>
  autoplot() +
  labs(title = "Residuals from the naïve method")




```
Easy way by single function -> gg_tsresiduals()

```{r}

google_2015 |>
  model(NAIVE(Close)) |>
  gg_tsresiduals()

```


```{r Portmanteau tests for autocorrelation}

# A test for a group of autocorrelations is called a portmanteau test

# Box-Pierce
aug |> features(.innov, box_pierce, lag = 10)

# Ljung Box
aug |> features(.innov, ljung_box, lag = 10)

# 
# p-value > 0.05 -> For both  
# Q and Q∗, the results are not significant (i.e., the  p-values are relatively large).
# Thus, we can conclude that the residuals are not distinguishable from a white noise series

# Box-Pierce 
# 
# NUll : Ho : a time series is white noise
#        so, if p-value<0.05 -> Reject NUll i.e. it is white noise...meaning It is not White Noise
#       Now, for p-value > 0.05 -> The series is Almost White Noise (NUll cannot be rejected)
# 
```


```{r}
# Drift Method

# An alternative simple approach that may be appropriate for forecasting the Google daily closing stock price is the drift method. The tidy() function shows the one estimated parameter, the drift coefficient, measuring the average daily change observed in the historical data.


fit <- google_2015 |> model(RW(Close ~ drift()))

augment(fit) |> features(.innov, ljung_box, lag=10)

# As with the naïve method, the residuals from the drift method are indistinguishable from a white noise series.

```


```{r Forecast distributions + Prediction Intervals}

# #####################################################################################################
# 
#  hilo():    The hilo() function converts the forecast distributions into intervals.
#             By default, 80% and 95% prediction intervals are returned, although 
#             other options are possible via the level argument
# 
# #####################################################################################################

google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  hilo()

# No level
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015, level=NULL) +                           # default level is 80 and 95%
  labs(title="Google daily closing stock price", y="$US" )

# level by default
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015) +
  labs(title="Google daily closing stock price", y="$US" )

# passing levels
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015, level=c(85, 90, 95, 99)) +
  labs(title="Google daily closing stock price", y="$US" )


```
#### Prediction Interval fro mBootstrapped Samples

When a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance

```{r Prediction Interval fro mBootstrapped Samples}

fit <- google_2015 |>
  model(NAIVE(Close))

simulate <- fit |> generate(h = 30, times = 5, bootstrap = TRUE)
simulate

# Here we have generated five possible sample paths for the next 30 trading days. The .rep variable provides a new key for the tsibble.

# we generated 5 sample paths so have index of values for each path too

google_2015 |>
  ggplot(aes(x = day)) +
  geom_line(aes(y = Close)) +
  geom_line(
    data = simulate,
    aes(y = .sim, colour = as.factor(.rep))) +
  labs(title="Google daily closing stock price", y="$US" ) +
  guides(colour = "none")


```
#### Shortcut for bootstrap (in forecast())

```{r}

fc <- fit |> forecast(h = 30, bootstrap = TRUE)
fc

# but now we have 5000 sample paths :)

autoplot(fc, google_2015) +
  labs(title="Google daily closing stock price", y="$US" )

# note how smart autoplot is here :)

```


```{r}

fc1000 <- 
  google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10, bootstrap = TRUE, times = 1000) |>
  hilo()

str(fc1000$.mean)

# 
# autoplot(fc1000, google_2015)+
#   labs(title = "Google daily stock closing price", y = "$US")
# Plot won't work with hilo here ..........
# 
# autoplot() tries to plot .mean (which is length 10) but also tries to handle those new hilo interval columns.
# 
# Since those intervals are stored as list objects, ggplot2 fails when mapping aesthetics because it expects numeric vectors, not lists.
# 
# This leads to the error about aesthetics length mismatch.

```


```{r Forecasting with transformations}

prices # commodity prices

fc <- prices |>
  filter(!is.na(eggs)) |>                  # removing NA
  model(RW(log(eggs) ~ drift())) |>        # log transfrom of prices and a drift model
  forecast(h = 50) |>                      # forecast for 50 horizons
  mutate(.median = median(eggs))           # new column with median value of eggs from the distribution in new fc

fc

fc |>
  autoplot(prices |> filter(!is.na(eggs)), level = c(80, 90, 95)) +
  geom_line(
    aes(y = .median),
    data = fc,
    linetype = 2,
    col = "blue") +
  labs(title = "Annual egg prices",
       y = "$US (in cents adjusted for inflation) ")


```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```











```{r}
# Plot one time series
aus_retail 
# |>
#   filter(`Series ID`=="A3349640L") |>
#   autoplot(Turnover)



```

```{r}

```

```{r}

# Plot one time series
# aus_retail |>
#   filter(`Series ID`=="A3349640L") |>
#   autoplot(Turnover)

aus_retail <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/aus_retail.csv") |>
  mutate(Month = yearmonth(Month)) |>
  as_tsibble(index = Month, key = c(`Series ID`))


```

```{r}

```
